{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#//----------------------------\n",
    "#//LIBRARIES\n",
    "    #Math\n",
    "import math\n",
    "    #Numeric Python\n",
    "import numpy as np\n",
    "    #Pandas (dataframes)\n",
    "import pandas as pd\n",
    "    #datetime for fate manipulation\n",
    "from datetime import date, datetime, timedelta  \n",
    "    #Regex for advanced string matching\n",
    "import re\n",
    "    #for time related stuff\n",
    "import time\n",
    "    #json library\n",
    "import json\n",
    "    #Analyst tools\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "from analysts_tools.growth import *\n",
    "    #Procurement tools\n",
    "from procurement_lib import send_slack_notification\n",
    "from procurement_lib import redash\n",
    "from analystcommunity.read_connection_data_warehouse import run_read_dwd_query\n",
    "from analysts_tools.redash_methods import *\n",
    "\n",
    "import random\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.signal import savgol_filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n"
     ]
    }
   ],
   "source": [
    "df_eagle = get_fresh_query_result(\"https://internal-redash.federate.frubana.com/\",177832,'SeoGHWmDUaaBi7VXje1s9zYNiMD1VHQ1K1DYOxiF',{},20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n"
     ]
    }
   ],
   "source": [
    "df_offers = get_fresh_query_result(\"https://internal-redash.federate.frubana.com/\",177833,'SeoGHWmDUaaBi7VXje1s9zYNiMD1VHQ1K1DYOxiF',{},20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n"
     ]
    }
   ],
   "source": [
    "df_bench = get_fresh_query_result(\"https://internal-redash.federate.frubana.com/\",179153,'SeoGHWmDUaaBi7VXje1s9zYNiMD1VHQ1K1DYOxiF',{},20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "SELECT DISTINCT \n",
    "  --qp.id,\n",
    "  --qp.name,\n",
    "  p.region_code,\n",
    "  --p.name,\n",
    "  p.name AS sku_name,\n",
    "  bs2.name AS card_name,\n",
    "  bs.addl_product_id AS card_id,\n",
    "  p.product_id AS product_id,\n",
    "  bs.sku_id,\n",
    "  p.sku,\n",
    "  bcat2.name as cat,\n",
    "  bcat.name as subcat\n",
    "FROM postgres_main_co.\"purchase_orders.products\"                        p\n",
    "INNER JOIN postgres_broadleaf_federate.\"broadleaf.blc_sku\"              bs      ON bs.upc = p.sku\n",
    "LEFT JOIN postgres_broadleaf_federate.\"broadleaf.blc_product\"           bp      ON bp.product_id = bs.addl_product_id --Conectar sku con la tarjeta\n",
    "LEFT JOIN postgres_broadleaf_federate.\"broadleaf.blc_sku\"               bs2     ON bs2.sku_id = bp.default_sku_id --Conectar la tarjeta con el sku que guarda la info\n",
    "LEFT JOIN postgres_broadleaf_federate.\"broadleaf.blc_category_xref\"     bcx     ON bcx.sub_category_id = bp.default_category_id AND bcx.archived='N' AND bcx.sndbx_tier is NULL --Relaciones categorias \n",
    "LEFT JOIN postgres_broadleaf_federate.\"broadleaf.blc_category\"          bcat    ON bp.default_category_id = bcat.category_id --Nombre subcategoria\n",
    "LEFT JOIN postgres_broadleaf_federate.\"broadleaf.blc_category\"          bcat2   ON bcx.category_id = bcat2.category_id --Nombre categoria\n",
    "\n",
    "where\n",
    "  p.deleted_at is null\n",
    "  and p.parent_id is null\n",
    "  and bs.archived = 'N'\n",
    "  and bs.sndbx_id is null\n",
    "  and bp.archived = 'N'\n",
    "  and bs.catalog_disc < 0\n",
    "  and ((bp.archived = 'N' and bp.sndbx_id is null) or (bp.archived is null))\n",
    "  \"\"\"\n",
    "df_aux1 = read_connection_data_warehouse.runQuery(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "SELECT  close_date\n",
    ",       region_code\n",
    ",       product_id\n",
    ",CASE\n",
    "    WHEN region_code IN ('SPO','BHZ','CWB','VCP') THEN SUM(stockout_correction * avg_price_without_discount)/4.75\n",
    "    WHEN region_code IN ('BOG','BAQ','MDE') THEN SUM(stockout_correction * avg_price_without_discount)/3776\n",
    "    WHEN region_code IN ('CMX') THEN SUM(stockout_correction * avg_price_without_discount)/19.65\n",
    "    ELSE SUM(stockout_correction * avg_price_without_discount)\n",
    "END AS gmv_lost\n",
    "FROM    lnd_ops.dp_input_historical_product_information\n",
    "WHERE   close_date >= CURRENT_DATE - 60\n",
    "        AND warehouse <> 'ALL'\n",
    "        --AND region_code = 'BOG'\n",
    "GROUP BY 1, 2, 3\n",
    "HAVING  SUM(stockout_correction * avg_price_without_discount) > 0\n",
    "  \"\"\"\n",
    "df_so = read_connection_data_warehouse.run_read_prod_query(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "query =  \"\"\"\n",
    "SELECT DISTINCT\n",
    "    s.identifier_value AS region_code,\n",
    "    DATE(fs.order_submitted_date) AS submit_date,\n",
    "    cat.parent_description as category,\n",
    "    cat.description AS subcategory,\n",
    "    dp.card_id,\n",
    "    dp.card_description AS product_name,\n",
    "    AVG(fs.product_price + (fs.product_tax/fs.product_quantity_x_step_unit)) AS product_price,\n",
    "    AVG(product_price_discount + (fs.product_tax/fs.product_quantity_x_step_unit)) AS product_price_discount,\n",
    "    AVG(pb.price) AS benchmark\n",
    "    \n",
    "FROM dpr_sales.fact_sales                   fs\n",
    "INNER JOIN dpr_shared.dim_customer          dc  ON dc.customer_id = fs.dim_customer\n",
    "INNER JOIN dpr_shared.dim_site              s   ON s.site_id = fs.dim_site\n",
    "INNER JOIN dpr_shared.dim_product           dp  ON dp.product_id = fs.dim_product\n",
    "INNER JOIN dpr_shared.dim_category          cat ON cat.category_id = dp.category_id\n",
    "LEFT JOIN dpr_sales.fact_sales_discounts    fsd ON fs.order_item_id = fsd.order_item_id  \n",
    "INNER JOIN dpr_shared.dim_stock_unit         su  ON su.card_id = dp.card_id\n",
    "INNER JOIN dpr_pricing.snap_daily_benchmark_competitor pb ON pb.dim_stock_unit = su.stock_unit_id AND fs.dim_submitted_date = pb.dim_benchmark_day\n",
    "\n",
    "WHERE \n",
    "    fs.gmv_enabled = TRUE\n",
    "    AND fulfillment_order_status NOT IN ('CANCELLED', 'ARCHIVED','No value')\n",
    "    AND fs.fb_order_status_id  IN (1,6,7,8)\n",
    "    AND fs.is_deleted = FALSE\n",
    "    AND fs.dim_status = 1\n",
    "    AND dp.is_slot = 'false'\n",
    "    AND fs.product_quantity_x_step_unit > 0\n",
    "    AND DATE(fs.order_submitted_date) >= DATE_TRUNC('day', date(getdate()) - interval '60 day')\n",
    "GROUP BY 1,2,3,4,5,6\n",
    "\"\"\"\n",
    "data_ventas_aux = run_read_dwd_query(query)\n",
    "data_ventas_aux[[\"product_price\", \"product_price_discount\"]] = data_ventas_aux[[\"product_price\", \"product_price_discount\"]].astype(float)\n",
    "data_ventas_aux['submit_date'] = pd.to_datetime(data_ventas_aux['submit_date']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_so = pd.merge(df_so, df_aux1[['region_code','product_id','card_id']],  how='left', left_on=['region_code','product_id'], right_on = ['region_code','product_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.merge(df_eagle, df_aux1[['region_code','product_id','card_id','sku_name','cat','subcat']],  how='left', left_on=['region_code','product_id'], right_on = ['region_code','product_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming df is your DataFrame\n",
    "def calculate_fit_for_discount(row):\n",
    "    if row['cat'] in ['Frutas e Verduras', 'Frutas & Verduras']:\n",
    "        if row['clearence_days'] < 1:\n",
    "            return 'FALSE'\n",
    "        elif 1 <= row['clearence_days'] < 2:\n",
    "            return 'check_with_supply'\n",
    "        else:\n",
    "            return 'TRUE'\n",
    "    elif row['cat'] in ['Abarrotes', 'Abarrotes & Despensa', 'Mercearia', 'Bebidas', 'Congelados', 'Despensa']:\n",
    "        if row['clearence_days'] <= 3:\n",
    "            return 'FALSE'\n",
    "        elif 3 < row['clearence_days'] <= 5:\n",
    "            return 'check_with_supply'\n",
    "        else:\n",
    "            return 'TRUE'\n",
    "    elif row['cat'] in ['Lácteos & Huevos', 'Lácteos Y Huevos', 'Laticínios e Ovos', 'Carne, Pollo & Pescados', 'Carnes, Aves e Peixes']:\n",
    "        if row['clearence_days'] <= 1:\n",
    "            return 'FALSE'\n",
    "        elif 1 < row['clearence_days'] <= 3:\n",
    "            return 'check_with_supply'\n",
    "        else:\n",
    "            return 'TRUE'\n",
    "    elif row['cat'] in ['Aseo e Higiene', 'Desechables', 'Descartáveis', 'Limpeza e Higiene']:\n",
    "        if row['clearence_days'] <= 2:\n",
    "            return 'FALSE'\n",
    "        elif 2 < row['clearence_days'] <= 4:\n",
    "            return 'check_with_supply'\n",
    "        else:\n",
    "            return 'TRUE'\n",
    "    else:\n",
    "        return 'FALSE'\n",
    "\n",
    "df['fit_for_discount'] = df.apply(calculate_fit_for_discount, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert date columns to a common format\n",
    "df_so['close_date'] = pd.to_datetime(df_so['close_date'])\n",
    "df['operation_date'] = pd.to_datetime(df['operation_date'])\n",
    "df_bench['fecha'] = pd.to_datetime(df_bench['fecha'])\n",
    "df_offers['created_at'] = pd.to_datetime(df_offers['created_at'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = pd.merge(df_offers, df[['region_code','operation_date','card_id','clearence_days','fit_for_discount']],  how='left', left_on=['city','created_at','card_id'], right_on = ['region_code','operation_date','card_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = df3.dropna().reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import timedelta\n",
    "\n",
    "# Assuming df3 is your offers DataFrame and df_so is your stock DataFrame\n",
    "\n",
    "# Convert date columns to datetime format\n",
    "df3['created_at'] = pd.to_datetime(df3['created_at'])\n",
    "df_so['close_date'] = pd.to_datetime(df_so['close_date'])\n",
    "\n",
    "# Perform the left join\n",
    "merged_df = df3.merge(df_so[['card_id', 'close_date', 'gmv_lost']], how='left', on='card_id')\n",
    "\n",
    "# Add condition to create null values for rows that don't match the condition\n",
    "merged_df.loc[\n",
    "    ~(\n",
    "        (merged_df['close_date'] >= merged_df['created_at']) &\n",
    "        (merged_df['close_date'] <= merged_df['created_at'] + timedelta(days=5))\n",
    "    ),\n",
    "    ['close_date', 'gmv_lost']\n",
    "] = [None, None]\n",
    "\n",
    "\n",
    "merged_df['close_date'] = merged_df['close_date'].fillna(merged_df['created_at'])\n",
    "merged_df['gmv_lost'] = merged_df['gmv_lost'].fillna(0)\n",
    "#Filter rows based on conditions\n",
    "merged_df = merged_df[\n",
    "    ((merged_df['close_date'] >= merged_df['created_at']) &\n",
    "     (merged_df['close_date'] <= merged_df['created_at'] + timedelta(days=5)))\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = merged_df.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom function to count non-zero and non-null values\n",
    "def custom_count(series):\n",
    "    return len([x for x in series if pd.notna(x) and x != 0])\n",
    "\n",
    "# Assuming 'merged_df' is your DataFrame\n",
    "result_df = merged_df.groupby(by=['city', 'responsable', 'created_at',\n",
    "       'card_id', 'name', 'cat', 'subcat', 'region_code', 'operation_date',\n",
    "       'clearence_days', 'fit_for_discount']).agg({'gmv_lost': [np.sum, custom_count]}).reset_index()\n",
    "\n",
    "# Rename columns for clarity\n",
    "result_df.columns = ['city', 'responsable', 'created_at',\n",
    "       'card_id', 'name', 'cat', 'subcat', 'region_code', 'operation_date',\n",
    "       'clearence_days', 'fit_for_discount', 'total_gmv_lost', 'count_non_zero']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_ventas_aux[\"dct\"] = 1-(data_ventas_aux.product_price_discount/data_ventas_aux.product_price)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.merge(data_ventas_aux, df_bench,  how='left', left_on=['region_code','submit_date','card_id'], right_on = ['region_code','fecha','card_id'])\n",
    "df2['bench'] = np.where(df2['benchmark_y'].notnull(), df2['benchmark_y'], df2['benchmark_x'])\n",
    "df2['gpi'] = df2.product_price/df2.bench\n",
    "df2['npi'] = df2.product_price_discount/df2.bench\n",
    "columns_to_drop = ['benchmark_x', 'benchmark_y', 'fecha']\n",
    "df2.drop(columns=columns_to_drop, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df2 = df2.loc[(df2.gpi>0.6) & (df2.gpi<2)].sort_values(\"gpi\").reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming your DataFrame is named 'df2'\n",
    "# Sort the DataFrame by 'card_id' and 'submit_date' in ascending order\n",
    "df2.sort_values(by=['card_id', 'submit_date'], inplace=True)\n",
    "\n",
    "# Calculate the percentage change for 'gpi', 'dct', and 'npi'\n",
    "df2['gpi_percent_change'] = df2.groupby('card_id')['gpi'].pct_change() #* 100\n",
    "df2['dct_percent_change'] = df2.groupby('card_id')['dct'].pct_change() #* 100\n",
    "df2['npi_percent_change'] = df2.groupby('card_id')['npi'].pct_change() #* 100\n",
    "\n",
    "# Drop the first row for each 'card_id' since percentage change is not available\n",
    "df2 = df2.dropna()\n",
    "\n",
    "# Reset the index if needed\n",
    "df2.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the rolling average for each card_id and submit_date\n",
    "rolling_period = 3  # Adjust this value as needed\n",
    "\n",
    "for col in ['gpi_percent_change', 'dct_percent_change', 'npi_percent_change']:\n",
    "    df2[f'{col}_avg_t-3'] = df2.groupby('card_id')[col].transform(lambda x: x.rolling(rolling_period).mean())\n",
    "    df2[f'{col}_avg_t+3'] = df2.groupby('card_id')[col].transform(lambda x: x.shift(-2).rolling(rolling_period).mean())\n",
    "    \n",
    "df2 = df2.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.to_excel(\"C2.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final = pd.merge(result_df, df2[['region_code','submit_date','card_id','gpi_percent_change_avg_t-3',\n",
    "       'gpi_percent_change_avg_t+3', 'dct_percent_change_avg_t-3',\n",
    "       'dct_percent_change_avg_t+3', 'npi_percent_change_avg_t-3',\n",
    "       'npi_percent_change_avg_t+3']],  how='left', left_on=['city','created_at','card_id'], right_on = ['region_code','submit_date','card_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final = df_final.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final.to_excel(\"B2.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df.to_excel(\"A2.xlsx\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
