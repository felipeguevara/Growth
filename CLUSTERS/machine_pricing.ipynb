{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Library & INFO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#//LIBRARIES\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import sys\n",
    "import os \n",
    "sys.path.append(os.path.expanduser('~'))\n",
    "\n",
    "from analysts_tools.growth import *\n",
    "\n",
    "#Procurement tools\n",
    "from procurement_lib import send_slack_notification,GoogleSheet,redash\n",
    "from analystcommunity.read_connection_data_warehouse import run_read_dwd_query\n",
    "\n",
    "from datetime import datetime, timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "city = 'SPO'\n",
    "\n",
    "todays_date = datetime.today().strftime('%Y-%m-%d')\n",
    "todays_date = pd.to_datetime(todays_date, format='%Y-%m-%d')\n",
    "todays_date = pd.Timestamp(todays_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sheet=GoogleSheet(\"1P8vmNi-_t7BL0i7jjX2fpfnOuKYqqtrBTmqo3Qr3cfg\")\n",
    "df_scrapper = df_sheet.get_as_dataframe('SCR. ATDO')\n",
    "df_assai_sht = df_sheet.get_as_dataframe('ASSAI')\n",
    "df_fix_prov = df_sheet.get_as_dataframe('FIX INDEX')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Informacion descriptiva de los productos prendidos en PAGINA\n",
    "query = \"\"\"\n",
    "WITH \n",
    "cluster AS (\n",
    "SELECT \n",
    "    sup.source_id,\n",
    "    type AS cluster,\n",
    "    ROW_NUMBER() OVER (PARTITION BY sup.source_id ORDER BY c.last_modified_at DESC, cluster DESC) AS rn\n",
    "FROM dpr_product_pricing.dim_sku_cluster_period c\n",
    "INNER JOIN dpr_shared.dim_stock_unit        su  ON su.sku = c.sku\n",
    "INNER JOIN dpr_shared.dim_stock_unit        sup  ON nvl(nullif(su.source_parent_id,0),su.source_id) = sup.source_id\n",
    "\n",
    "WHERE c.site_id in (4,6,9,11)\n",
    " AND su.active = 1\n",
    " AND su.archived = 0\n",
    " AND su.in_catalog = 1\n",
    " ),\n",
    "\n",
    "skus AS (\n",
    "SELECT\n",
    "    s.identifier_value AS city,\n",
    "        CASE WHEN cat.parent_description = 'Mercearia' \n",
    "         AND cat.description NOT IN ('Arroz', 'Açúcar, adoçantes e doces','Açúcar e adoçantes','Feijão','Grãos','Farinhas e misturas','Azeites, óleos e vinagres')\n",
    "         THEN 'Despensa'\n",
    "        ELSE cat.parent_description \n",
    "    END AS cat,\n",
    "    cat.description AS subcat,\n",
    "    COALESCE(clt.cluster,'TAIL') AS cluster,\n",
    "    sup.source_id,\n",
    "    sup.description\n",
    "FROM dpr_shared.dim_stock_unit          su\n",
    "INNER JOIN dpr_shared.dim_stock_unit    sup ON nvl(nullif(su.source_parent_id,0),su.source_id) = sup.source_id AND su.active = 1 AND su.archived = 0 AND su.in_catalog = 1\n",
    "INNER JOIN dpr_shared.dim_site          s   ON s.site_id = sup.site_id\n",
    "INNER JOIN dpr_shared.dim_category      cat ON cat.category_id = sup.category_id AND cat.super_category = 'Multicategoría'\n",
    "LEFT JOIN cluster                       clt ON clt.source_id = sup.source_id AND clt.rn = 1\n",
    "WHERE city IN ('SPO','CWB','BHZ','VCP')\n",
    "),\n",
    "\n",
    "penetracion AS (\n",
    "SELECT\n",
    "s.identifier_value AS region,\n",
    "COUNT(DISTINCT fs.dim_customer) AS total_custom \n",
    "FROM dpr_sales.fact_sales                   fs\n",
    "INNER JOIN dpr_shared.dim_site              s   ON s.site_id = fs.dim_site\n",
    "INNER JOIN dpr_shared.dim_product           dp  ON dp.product_id = fs.dim_product\n",
    "INNER JOIN dpr_shared.dim_category          cat ON cat.category_id = dp.category_id\n",
    "\n",
    "WHERE \n",
    "    fs.gmv_enabled = TRUE\n",
    "    AND cat.super_category = 'Multicategoría'\n",
    "    AND fulfillment_order_status NOT IN ('CANCELLED', 'ARCHIVED','No value')\n",
    "    AND fs.fb_order_status_id IN (1,6,7,8)\n",
    "    AND fs.is_deleted = FALSE\n",
    "    AND fs.dim_status = 1\n",
    "    AND dp.is_slot = 'false'\n",
    "    AND fs.gmv_pxq_local > 0\n",
    "    AND s.identifier_value IN ('SPO','CWB','VCP','BHZ')\n",
    "    AND DATE(fs.order_submitted_date) >= CURRENT_DATE - 14\n",
    "GROUP BY s.identifier_value\n",
    "),\n",
    "\n",
    "sales AS (\n",
    "SELECT\n",
    "    --DATE(fs.order_submitted_date) AS fecha,\n",
    "    sup.source_id,\n",
    "    100.00*COUNT(DISTINCT fs.dim_customer)::FLOAT/AVG(p.total_custom) AS penet,\n",
    "    SUM(fs.product_quantity_x_step_unit) AS cant,\n",
    "    SUM(fs.gmv_pxq_local)/4.75 AS gmv_usd,\n",
    "    --SUM(COALESCE(fsd.product_discount,0))/4.75 AS dct_usd,\n",
    "    --dct_usd/gmv_usd AS per_dct,\n",
    "    --AVG(COALESCE(inventory_p_fin,cogs_p_mtd)) AS costo,\n",
    "    -- gmv_usd*margin/100.00 AS cash_margin,\n",
    "    -- cash_margin-dct_usd AS net_cash_margin,\n",
    "    -- 100.00*net_cash_margin/gmv_usd AS net_margin,\n",
    "    100.00*gmv_usd/SUM(gmv_usd) OVER (PARTITION BY s.identifier_value) AS gmv_mix--,\n",
    "    -- 100.00*SUM(fs.product_price*fs.product_quantity_x_step_unit)/SUM(min_price*fs.product_quantity_x_step_unit) AS gpi,\n",
    "    -- 100.00*SUM(fs.product_price_discount*fs.product_quantity_x_step_unit)/SUM(min_price*fs.product_quantity_x_step_unit) AS npi\n",
    "\n",
    "    \n",
    "FROM dpr_sales.fact_sales                   fs\n",
    "--INNER JOIN dpr_shared.dim_customer          dc  ON dc.customer_id = fs.dim_customer\n",
    "INNER JOIN dpr_shared.dim_site              s   ON s.site_id = fs.dim_site\n",
    "INNER JOIN dpr_shared.dim_product           dp  ON dp.product_id = fs.dim_product\n",
    "INNER JOIN dpr_shared.dim_category          cat ON cat.category_id = dp.category_id\n",
    "--LEFT JOIN dpr_sales.fact_sales_discounts    fsd ON fs.order_item_id = fsd.order_item_id \n",
    "INNER JOIN dpr_shared.dim_stock_unit        su  ON su.product_id = fs.dim_product\n",
    "INNER JOIN dpr_shared.dim_stock_unit        sup ON nvl(nullif(su.source_parent_id,0),su.source_id) = sup.source_id\n",
    "INNER JOIN penetracion                      p   ON p.region = s.identifier_value\n",
    "--LEFT JOIN dpr_cross_business.fact_cross_business_insights m ON m.dim_stock_unit = sup.stock_unit_id AND m.dim_date = fs.dim_submitted_date AND DATE(fs.order_submitted_date) = DATE(current_date)\n",
    "--LEFT JOIN dpr_product_pricing.obt_benchmark_product_prices  pb  ON pb.stock_unit_id = sup.stock_unit_id AND DATE(fs.order_submitted_date) = pb.benchmark_date\n",
    "\n",
    "WHERE \n",
    "    fs.gmv_enabled = TRUE\n",
    "    AND cat.super_category = 'Multicategoría'\n",
    "    AND fulfillment_order_status NOT IN ('CANCELLED', 'ARCHIVED','No value')\n",
    "    AND fs.fb_order_status_id IN (1,6,7,8)\n",
    "    AND fs.is_deleted = FALSE\n",
    "    AND fs.dim_status = 1\n",
    "    AND dp.is_slot = 'false'\n",
    "    AND fs.gmv_pxq_local > 0\n",
    "    --AND (cogs_p_mtd > 0 OR inventory_p_fin > 0)\n",
    "    AND s.identifier_value IN ('SPO','CWB','VCP','BHZ')\n",
    "    AND DATE(fs.order_submitted_date) >= CURRENT_DATE - 14\n",
    "GROUP BY 1,s.identifier_value\n",
    ")\n",
    "\n",
    "SELECT \n",
    "    skus.*,\n",
    "    penet::FLOAT,\n",
    "    cant::FLOAT,\n",
    "    gmv_usd::FLOAT,\n",
    "    gmv_mix::FLOAT\n",
    "FROM skus\n",
    "LEFT JOIN sales ON skus.source_id = sales.source_id\"\"\"\n",
    "dfq1 = run_read_dwd_query(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Informacion de precios y costos de los productos prendidos en PAGINA\n",
    "query = \"\"\"\n",
    "SELECT \n",
    "sup.source_id,\n",
    "--sup.description,\n",
    "--p.min_base_price,\n",
    "p.min_gross_price::FLOAT AS price,\n",
    "p.min_pricing_price::FLOAT AS net_price,\n",
    "--p.min_sale_price,\n",
    "COALESCE(CASE WHEN dtd_cost_local = 0 THEN inventory_p_fin ELSE dtd_cost_local END,(1-gross_margin/100)*min_gross_price)::FLOAT AS cost,\n",
    "-- ((1-gross_margin/100)*min_gross_price)::FLOAT AS cost,\n",
    "COALESCE(100.00*(1-(cost/price)),gross_margin)::FLOAT AS mg,\n",
    "COALESCE(100.00*(1-(cost/net_price)),net_pricing_margin)::FLOAT AS nmg,\n",
    "discount_pricing_value::FLOAT AS dct\n",
    "\n",
    "FROM dpr_product_pricing.dim_product_current_price p\n",
    "INNER JOIN dpr_shared.dim_stock_unit        su  ON su.stock_unit_id = p.stock_unit_id\n",
    "INNER JOIN dpr_shared.dim_stock_unit        sup ON nvl(nullif(su.source_parent_id,0),su.source_id) = sup.source_id\n",
    "INNER JOIN dpr_shared.dim_category          cat ON cat.category_id = sup.category_id AND cat.super_category = 'Multicategoría'\n",
    "LEFT JOIN  dpr_cross_business.fact_cross_business_insights m ON m.dim_stock_unit = sup.stock_unit_id AND m.dim_date = TO_CHAR(current_date,'YYYYMMDD')::INT\n",
    "LEFT JOIN  dpr_cross_business.int_dtd_cost dt ON dt.dim_stock_unit = sup.stock_unit_id AND dt.dim_date_dtd = TO_CHAR(current_date,'YYYYMMDD')::INT\n",
    "WHERE p.site_id in (4,6,9,11)\n",
    " AND su.active = 1\n",
    " AND su.archived = 0\n",
    " AND su.in_catalog = 1\n",
    " \"\"\"\n",
    "dfq2 = run_read_dwd_query(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Info Offers Dct. Proveedor\n",
    "# query = \"\"\"\n",
    "# SELECT\n",
    "#   s.identifier_value AS city,\n",
    "#   dof.source_id AS offer_id,\n",
    "#   dof.description AS offer_name,\n",
    "#   dot.description AS offer_type,\n",
    "# --   fso.date_created AS date_created,\n",
    "# --   fso.date_updated AS date_updated,\n",
    "# --   duc.\"name\" AS user_creator,\n",
    "# --   duu.name AS user_updater,\n",
    "#   fso.start_date AS start_date,\n",
    "#   fso.end_date AS end_date,\n",
    "#   fso.discount,\n",
    "#   fso.max_uses_per_order,\n",
    "#   fso.max_uses_per_customer,\n",
    "#   fso.use_segment_to_exclude,\n",
    "#   fso.customer_segment_id,\n",
    "#   cat.parent_description AS cat,\n",
    "#   cat.description AS subcat,\n",
    "#   su.source_id,\n",
    "#   su.card_Description AS product\n",
    "    \n",
    "# FROM dpr_sales.fact_sales_offers        fso -- Shared dimensions\n",
    "# INNER JOIN dpr_shared.dim_date              ON dim_date.date_id = fso.dim_date_created\n",
    "# INNER JOIN dpr_shared.dim_site          s   ON s.site_id = fso.dim_site\n",
    "# --INNER JOIN dpr_shared.dim_user_admin duc ON duc.user_admin_id = fso.dim_user_creator\n",
    "# --INNER JOIN dpr_shared.dim_user_admin duu ON duu.user_admin_id = fso.dim_user_updater -- Model dimensions\n",
    "# INNER JOIN dpr_sales.dim_offer          dof ON dof.offer_id = fso.dim_offer\n",
    "# INNER JOIN dpr_sales.dim_offer_type     dot ON dot.offer_type_id = fso.dim_offer_type\n",
    "# INNER JOIN dpr_shared.dim_stock_unit    su  ON fso.dim_stock_unit = su.stock_unit_id\n",
    "# INNER JOIN dpr_shared.dim_category      cat ON su.category_id = cat.category_id\n",
    "# --left join dpr_sales.dim_customers_segments dcs ON dcs.customer_segment_id = fso.customer_segment_id -- Historical changes\n",
    "\n",
    "# WHERE fso.automatically_added = 1\n",
    "#  AND fso.start_date <= current_date\n",
    "#  AND fso.end_date > current_date\n",
    "#  AND dof.description ILIKE 'ACMKTPLC%'\n",
    "#  \"\"\"\n",
    "# df_prov = run_read_dwd_query(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "select\n",
    "    competitor.competitor_name AS competitor_name,   \n",
    "    site.identifier_value as site_code,\n",
    "    quotation_date.full_date AS quotation_date,\n",
    "    su.source_id,\n",
    "    ROUND(cpp.product_selected_price,2)::float as price--ROUND(MEDIAN(cpp.product_selected_price),2)::float as price\n",
    "from dpr_product_pricing.fact_collected_product_prices cpp\n",
    "    inner join dpr_shared.dim_date quotation_date\n",
    "        on cpp.dim_quotation_date = quotation_date.date_id\n",
    "    inner join dpr_shared.dim_time quotation_time\n",
    "        on cpp.dim_quotation_time = quotation_time.time_id\n",
    "    inner join dpr_shared.dim_site site\n",
    "        on cpp.dim_site = site.site_id\n",
    "    inner join dpr_shared.dim_category cat\n",
    "        on cpp.dim_category = cat.category_id\n",
    "    inner join dpr_product_pricing.dim_product_outlier_type outlier_type\n",
    "        on cpp.dim_outlier_type = outlier_type.outlier_type_id\n",
    "    inner join dpr_product_pricing.dim_product_source_type source_type\n",
    "        on cpp.dim_source_type = source_type.source_type_id\n",
    "    inner join dpr_product_pricing.dim_product_competitor competitor\n",
    "        on cpp.dim_competitor = competitor.competitor_id\n",
    "    inner join dpr_product_pricing.dim_product_competitor_type competitor_type\n",
    "        on(\n",
    "            case\n",
    "                when cpp.super_category = 'Fruver'\n",
    "                    then competitor.product_competitor_type_id_fruver = competitor_type.competitor_type_id\n",
    "                when cpp.super_category = 'Multicategoría'\n",
    "                    then competitor.product_competitor_type_id_multicategoria = competitor_type.competitor_type_id\n",
    "            end\n",
    "        )\n",
    "    inner join dpr_shared.dim_stock_unit su\n",
    "        on cpp.dim_stock_unit = su.stock_unit_id\n",
    "where quotation_date.full_date >= current_date - 10\n",
    "    AND (\n",
    "        competitor.competitor_name NOT ILIKE '%cayena%'\n",
    "        AND (\n",
    "            competitor.competitor_name <> 'Atacadao_V2'\n",
    "            OR su.source_id IN {skus_scrapper}\n",
    "        )\n",
    "    )\n",
    "    AND site.identifier_value IN ('SPO')\n",
    "\n",
    "--GROUP BY 1,2,3,4\n",
    "\"\"\".format(skus_scrapper = tuple(list(df_scrapper.source_id.unique()) + [1]))\n",
    "df_zkkkkk = run_read_dwd_query(query)\n",
    "\n",
    "df_zkkkkk = df_zkkkkk.dropna().reset_index(drop=True)\n",
    "df_zkkkkk['lifetime'] = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_zkkkkk = df_zkkkkk.loc[df_zkkkkk.site_code==city].reset_index(drop=True).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure dataframe is sorted by 'quotation_date'\n",
    "df_zkkkkk = df_zkkkkk.sort_values(by='quotation_date')\n",
    "\n",
    "# Generate the required rows for missing dates\n",
    "new_rows = []\n",
    "\n",
    "for (competitor, source_id), group in df_zkkkkk.groupby(['competitor_name', 'source_id']):\n",
    "    group = group.sort_values(by='quotation_date')\n",
    "    last_known_price = None\n",
    "    last_known_date = None\n",
    "    lifetime = 8\n",
    "    \n",
    "    for current_index in range(len(group)):\n",
    "        current_date = group.iloc[current_index]['quotation_date']\n",
    "        price = group.iloc[current_index]['price']\n",
    "        \n",
    "        # If this is not the first iteration, fill in missing dates\n",
    "        if last_known_date is not None:\n",
    "            days_diff = (current_date - last_known_date).days\n",
    "            if days_diff > 1:\n",
    "                for j in range(1, min(days_diff, lifetime + 1)):\n",
    "                    new_date = last_known_date + timedelta(days=j)\n",
    "                    new_row = {\n",
    "                        'site_code': group.iloc[current_index]['site_code'],\n",
    "                        'quotation_date': new_date,\n",
    "                        'competitor_name': competitor,\n",
    "                        'source_id': source_id,\n",
    "                        'price': last_known_price,\n",
    "                        'lifetime': lifetime - j\n",
    "                    }\n",
    "                    new_rows.append(new_row)\n",
    "                    \n",
    "                    # Stop if we reach a new datapoint date\n",
    "                    if new_date + timedelta(days=1) == current_date:\n",
    "                        break\n",
    "        \n",
    "        # Update the last known values and reset lifetime\n",
    "        last_known_price = price\n",
    "        last_known_date = current_date\n",
    "        lifetime = 8  # Reset lifetime\n",
    "\n",
    "    # After processing all known dates for the group, continue generating rows until lifetime reaches 0\n",
    "    while lifetime > 0:\n",
    "        last_known_date += timedelta(days=1)\n",
    "        new_row = {\n",
    "            'site_code': group.iloc[-1]['site_code'],\n",
    "            'quotation_date': last_known_date,\n",
    "            'competitor_name': competitor,\n",
    "            'source_id': source_id,\n",
    "            'price': last_known_price,\n",
    "            'lifetime': lifetime - 1\n",
    "        }\n",
    "        new_rows.append(new_row)\n",
    "        lifetime -= 1\n",
    "\n",
    "# Append new rows to the dataframe\n",
    "df_zkkkkk = df_zkkkkk.append(new_rows, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.merge(dfq1.loc[dfq1.city == city],dfq2,left_on=['source_id'],right_on=['source_id'],how='inner')\n",
    "df = df.sort_values(by=['cat','subcat', 'cluster','gmv_mix'], ascending=[False, True, True,False]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bench = df_zkkkkk.loc[df_zkkkkk.quotation_date == todays_date.date()].reset_index(drop=True).copy()\n",
    "\n",
    "# Function to calculate the required statistics\n",
    "def calculate_statistics(df):\n",
    "    return df.groupby('source_id')['price'].agg(\n",
    "        num_data_points='count',\n",
    "        num_competitors=lambda x: df.loc[x.index, 'competitor_name'].nunique(),\n",
    "        min_price='min',\n",
    "        avg_price='mean',\n",
    "        median_price='median',\n",
    "        max_price='max',\n",
    "        #delta_min_max=lambda x: x.max() - x.min()\n",
    "    ).reset_index()\n",
    "\n",
    "# Calculate statistics for all competitors\n",
    "stats_all = calculate_statistics(df_bench)\n",
    "stats_all.columns = ['source_id', 'num_data_points_all', 'num_competitors_all', 'min_price_all', 'avg_price_all', 'Med ALL', 'max_price_all']\n",
    "\n",
    "# Filter for competitors that include \"assai\" in their name and calculate statistics\n",
    "df_assai = df_bench[df_bench['competitor_name'].str.contains('assaí', case=False, na=False)]\n",
    "stats_assai = calculate_statistics(df_assai)\n",
    "stats_assai.columns = ['source_id', 'num_data_points_assai', 'num_competitors_assai', 'min_price_assai', 'avg_price_assai', 'Med Assai', 'max_price_assai']\n",
    "\n",
    "# Filter for competitors that include \"atacadao\" or \"atacadão\" in their name and calculate statistics\n",
    "df_atacadao = df_bench[df_bench['competitor_name'].str.match(r'(?i)^atacad[aã]o') & ~df_bench['competitor_name'].str.contains(r'(?i)^Atacadao_V2$')]\n",
    "stats_atacadao = calculate_statistics(df_atacadao)\n",
    "stats_atacadao.columns = ['source_id', 'num_data_points_atacadao', 'num_competitors_atacadao', 'min_price_atacadao', 'avg_price_atacadao', 'Med Atacadao', 'max_price_atacadao']\n",
    "\n",
    "# Filter for competitors that include \"atacadao_v2\" the scrapper\n",
    "df_scrapper_atacadao = df_bench[df_bench['competitor_name'].str.contains(r'(?i)^Atacadao_V2$')]\n",
    "stats_scrapper_atacadao = calculate_statistics(df_scrapper_atacadao)\n",
    "stats_scrapper_atacadao.columns = ['source_id', 'num_data_points_atacadao_scrapper', 'num_competitors_atacadao_scrapper', 'Scrp. Atacadao', 'avg_price_atacadao_scrapper', 'Med atacadao_scrapper', 'max_price_atacadao_scrapper']\n",
    "\n",
    "# Merge the results\n",
    "bench_df = stats_all.merge(stats_assai, on='source_id', how='left').merge(stats_atacadao, on='source_id', how='left').merge(stats_scrapper_atacadao, on='source_id', how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PRINTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PRINT DESCRIPTIVE INFO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the manual info in the file\n",
    "df_info_sheet = df_sheet.get_as_dataframe('info_to_py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of columns to be divided by 100\n",
    "columns_to_divide = ['penet', 'gmv_mix', 'mg', 'nmg', 'dct']\n",
    "\n",
    "# Divide the specified columns by 100\n",
    "df_print = df.copy()\n",
    "df_print[columns_to_divide] = df_print[columns_to_divide] / 100\n",
    "\n",
    "# Se pegan los parametros de la estrategia establecida para mantenerse constante\n",
    "df_print = pd.merge(df_print,df_info_sheet,left_on=['source_id'],right_on=['ID'],how='left')\n",
    "df_print.drop(columns=['ID'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PEGAR INFO\n",
    "df_sheet.set_with_dataframe(df_print, worksheet='info', clear=True, autocreate=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PRINT BENCH INFO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aca dejamos las primeras 19 columnas porque no queremos incluir las columnas del SCRAPPER de Atacadao"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PEGAR INFO\n",
    "df_sheet.set_with_dataframe(bench_df.iloc[:, :19], worksheet='data', clear=True, autocreate=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PRINT MIN BENCH INFO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by source_id and get the row with the minimum price\n",
    "min_price_idx = df_bench.groupby('source_id')['price'].idxmin()\n",
    "\n",
    "# Use these indices to get the rows with the minimum price\n",
    "df_bench_min = df_bench.loc[min_price_idx, ['source_id', 'competitor_name', 'price']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PEGAR INFO\n",
    "df_sheet.set_with_dataframe(df_bench_min, worksheet='min_bench', clear=True, autocreate=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## APPLYING RULES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### APLICAR ESTRATEGIA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1. NEW OR NULL VALUES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paso 1: Encontrar las filas donde 'Strategy' es nulo\n",
    "na_rows = df_print[df_print['Strategy'].isna()]\n",
    "\n",
    "# Paso 2: Reemplazar valores nulos con valores predeterminados\n",
    "df_print.loc[na_rows.index, 'Tipo producto'] = df_print.loc[na_rows.index, 'Tipo producto'].fillna('Mais barato')\n",
    "df_print.loc[na_rows.index, 'Index'] = df_print.loc[na_rows.index, 'Index'].fillna(1)\n",
    "df_print.loc[na_rows.index, 'Frecuency'] = df_print.loc[na_rows.index, 'Frecuency'].fillna('Diaria')\n",
    "df_print.loc[na_rows.index, 'Strategy'] = df_print.loc[na_rows.index, 'Strategy'].fillna('Med Atacadao')\n",
    "\n",
    "# Paso 3: Calcular el promedio de 'Min margin' para cada subcategoría y usarlo para llenar los valores nulos en 'Min margin'\n",
    "avg_margin_by_subcat = df_print.groupby('subcat')['Min margin'].transform('mean')\n",
    "df_print.loc[na_rows.index, 'Min margin'] = df_print.loc[na_rows.index, 'Min margin'].fillna(avg_margin_by_subcat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stg = df_print.loc[~df_print['Strategy'].isna()].reset_index(drop=True).copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 Add Indexes Dct. Proveedor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For update Index\n",
    "df_fix_prov_2 = df_fix_prov.loc[(df_fix_prov.fecha_inicio <= str(todays_date.date())) & (df_fix_prov.fecha_fin > str(todays_date.date()))]\n",
    "# For turn back Index\n",
    "df_fix_prov_3 = df_fix_prov.loc[(pd.to_datetime(df_fix_prov.fecha_fin)) == todays_date + timedelta(days=-1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary from df_fix in order to get back the previous Index !!\n",
    "fix_index_dict_back = df_fix_prov_3.set_index('source_id')['INDEX ACTUAL'].to_dict()\n",
    "\n",
    "# Apply the updates\n",
    "mask_prov_back = df_stg['source_id'].isin(fix_index_dict_back.keys())\n",
    "df_stg.loc[mask_prov_back, 'Index'] = df_stg.loc[mask_prov_back, 'source_id'].map(fix_index_dict_back)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary from df_fix in order to get to the desired Index !!\n",
    "fix_index_dict = df_fix_prov_2.set_index('source_id')['INDEX FIJO'].to_dict()\n",
    "\n",
    "# Apply the updates\n",
    "mask_prov = df_stg['source_id'].isin(fix_index_dict.keys())\n",
    "df_stg.loc[mask_prov, 'Index'] = df_stg.loc[mask_prov, 'source_id'].map(fix_index_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Logic to define the price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def formula(row):\n",
    "    # Obtener la estrategia y el source_id\n",
    "    primary_strategy = row['Strategy']\n",
    "    source_id = row['source_id']\n",
    "    index = row['Index']\n",
    "\n",
    "    # Filtrar bench_df para el source_id específico\n",
    "    filtered_bench = bench_df[bench_df['source_id'] == source_id]\n",
    "    \n",
    "    # Inicializar bench_value\n",
    "    bench_value = None\n",
    "    \n",
    "    # Estrategias en orden de preferencia\n",
    "    strategies = ['Scrp. Atacadao','Med Atacadao', 'Med Assai', 'Med ALL']\n",
    "    \n",
    "    # Encontrar el índice de la estrategia primaria\n",
    "    if source_id in df_assai_sht.source_id.unique():\n",
    "        primary_index = 2\n",
    "    else:\n",
    "        primary_index = 0#strategies.index(primary_strategy)\n",
    "    \n",
    "    # Verificar la estrategia primaria y las siguientes en el orden de preferencia\n",
    "    for strategy in strategies[primary_index:]:\n",
    "        if not filtered_bench.empty and pd.notna(filtered_bench[strategy].values[0]):\n",
    "            bench_value = filtered_bench[strategy].values[0]\n",
    "            break\n",
    "    \n",
    "    # Si no se encontró un valor válido en las estrategias, usar el valor fallback\n",
    "    if bench_value is None:\n",
    "        if row['mg'] > row['Min margin']:\n",
    "            fallback_value = row['net_price']\n",
    "        else:\n",
    "            fallback_value = round((row['cost'] / (1 - row['Min margin']))*(1-row['dct']), 2) #incluimos dct\n",
    "        return 'Margin', None, fallback_value, 'NO BENCH'\n",
    "    \n",
    "\n",
    "    # New Bench Values Based On Index\n",
    "    bench_value = bench_value*index #Aca multiplica por el Index\n",
    "    # Calcular el nuevo margen\n",
    "    new_margin = 1 - (row['cost'] / (bench_value*(1+row['dct']))) #new margin gross\n",
    "    \n",
    "    # Comprobar si el nuevo margen es mayor que el margen mínimo\n",
    "    if new_margin > row['Min margin']:\n",
    "        # Si es así, retornar el valor de referencia como el nuevo precio y el bench_value como new_bench\n",
    "        return strategy, round(bench_value, 2), round(bench_value, 2), 'PRICED BENCH'\n",
    "    else:\n",
    "        # De lo contrario, retornar el valor fallback y el bench_value como new_bench\n",
    "        fallback_value = round((row['cost'] / (1 - row['Min margin']))*(1-row['dct']), 2)\n",
    "        return strategy, round(bench_value, 2), fallback_value, 'PRICED BY MIN MARGIN'\n",
    "\n",
    "# Aplicar la función a cada fila y desempaquetar los resultados en dos nuevas columnas\n",
    "df_stg[['Strategy','new_bench', 'new_price','explanation']] = df_stg.apply(lambda row: pd.Series(formula(row)), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Logic to apply elasticities (B. Suaves)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_info_ryd = df_sheet.get_as_dataframe('elasticity')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bs = pd.merge(df_stg,df_info_ryd,left_on=['source_id'],right_on=['source_id'],how='left')\n",
    "df_bs['uplift'] = 100.00*((df_bs.new_price/df_bs.net_price)-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import percentileofscore\n",
    "\n",
    "# Function to calculate new_uplift for each city\n",
    "def calculate_new_uplift(df):\n",
    "    # Calculate percentiles for each value in the mean_edpv column\n",
    "    percentiles = df['mean_edpv'].apply(lambda x: percentileofscore(df['mean_edpv'], x) / 100.0)\n",
    "    \n",
    "    # Apply the transformation (1 - percentile) and map it to the range [2, 10]\n",
    "    mapped_values = 4 + (1 - percentiles) * (10 - 4)\n",
    "    \n",
    "    # Calculate new_uplift by considering the sign of the original uplift\n",
    "    df['new_uplift'] = np.where(df['uplift'] >= 0, np.minimum(mapped_values, df['uplift']),\n",
    "                                np.maximum(-mapped_values, df['uplift']))\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the calculation for each city separately\n",
    "df_bs = df_bs.groupby('city').apply(calculate_new_uplift)\n",
    "\n",
    "# Handle rows with NaN in mean_edpv separately if needed\n",
    "df_bs_na = df_bs[df_bs['mean_edpv'].isna()].copy()\n",
    "\n",
    "# Adjust the assignment logic to consider the sign of uplift\n",
    "df_bs_na['new_uplift'] = np.where(df_bs_na.cluster == 'KVI', \n",
    "                                  np.where(df_bs_na.uplift >= 0, np.minimum(4, df_bs_na.uplift), np.maximum(-4, df_bs_na.uplift)),\n",
    "                       np.where(df_bs_na.cluster == 'MID', \n",
    "                                np.where(df_bs_na.uplift >= 0, np.minimum(7, df_bs_na.uplift), np.maximum(-7, df_bs_na.uplift)),\n",
    "                       np.where(df_bs_na.uplift >= 0, np.minimum(10, df_bs_na.uplift), np.maximum(-10, df_bs_na.uplift))))\n",
    "\n",
    "# Update the original DataFrame with new_uplift for NaN mean_edpv\n",
    "df_bs.loc[df_bs_na.index, 'new_uplift'] = df_bs_na['new_uplift']\n",
    "\n",
    "# Calculate new_price and round to 2 decimal places\n",
    "df_bs['new_price'] = (df_bs.net_price * (1 + df_bs.new_uplift / 100)).round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If bajadas suaves was applied\n",
    "df_bs['explanation'] = np.where((df_bs['uplift'] != df_bs['new_uplift']) & (df_bs['explanation'] != \"PRICED BY MIN MARGIN\"), \"B. SUAVES\", df_bs['explanation'])\n",
    "\n",
    "# New price with bajadas suaves\n",
    "df_stg['new_price'] = df_bs['new_price']\n",
    "\n",
    "# New price with bajadas suaves\n",
    "df_stg['explanation'] = df_bs['explanation']\n",
    "\n",
    "# Validate strategy of refresh\n",
    "df_stg['Frecuency'] = np.where(df_stg['cluster'] == 'KVI', 'Diaria', 'Semanal')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logica para redondear a .09 los final price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to round to nearest .09\n",
    "def round_to_nearest_09(value):\n",
    "    # Find the integer part and the fractional part\n",
    "    integer_part = np.floor(value)\n",
    "    fractional_part = value - integer_part\n",
    "\n",
    "    # Find the closest multiple of 0.09\n",
    "    rounded_fraction = round(fractional_part / 0.10) * 0.10 - 0.01\n",
    "    \n",
    "    # Combine the integer part with the rounded fractional part\n",
    "    return np.round(integer_part + rounded_fraction, 2)\n",
    "\n",
    "# Apply the function to the DataFrame\n",
    "df_stg['new_price'] = df_stg['new_price'].apply(round_to_nearest_09)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pegar EXPLANATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fix = df_sheet.get_as_dataframe('FIX PRICES')\n",
    "df_fix = df_fix.loc[(df_fix.fecha_inicio <= str(todays_date.date())) & (df_fix.fecha_fin > str(todays_date.date()))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary from df_fix for quick lookup\n",
    "fix_price_dict = df_fix.set_index('source_id')['precio_fijo'].to_dict()\n",
    "\n",
    "# Apply the updates\n",
    "mask = df_stg['source_id'].isin(fix_price_dict.keys())\n",
    "df_stg.loc[mask, 'new_price'] = df_stg.loc[mask, 'source_id'].map(fix_price_dict)\n",
    "df_stg.loc[mask, 'Strategy'] = 'Fix Price'\n",
    "df_stg.loc[mask, 'explanation'] = 'Fix Price CatMan'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aca Para las negociaciones dct proveedor\n",
    "df_stg.loc[mask_prov, 'Strategy'] = 'Negotiation Prov'\n",
    "df_stg.loc[mask_prov, 'explanation'] = 'Fix Price CatMan'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PEGAR INFO\n",
    "df_sheet.set_with_dataframe(df_stg.loc[:, ['source_id','explanation']], worksheet='explanation', clear=True, autocreate=True)\n",
    "\n",
    "# Seleccionar columnas de la 6 a la 9 (índices 5 a 8)\n",
    "# subset_columns = df_print.iloc[:, 5:9]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pegar INFO FINAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PEGAR INFO\n",
    "df_sheet.set_with_dataframe(df_stg.iloc[:, -8:], worksheet='info_to_sheet', clear=True, autocreate=True)\n",
    "\n",
    "# Seleccionar columnas de la 6 a la 9 (índices 5 a 8)\n",
    "# subset_columns = df_print.iloc[:, 5:9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =IFERROR(\n",
    "#   IFS(\n",
    "#     OR(\n",
    "#       AND(AC9<>\"\",OR(AC9>=75%,AC9<=-30%,L9/J9<15%))\n",
    "#     ), \"OUTLIER\",\n",
    "    \n",
    "#     OR(\n",
    "#       OR(R9=\"NO TOCAR\",U9=\"Fix Price\")\n",
    "#     ), \"FIX/NO TOCAR\",\n",
    "\n",
    "#     OR(\n",
    "#       AND(W9<>\"\", C9=\"KVI\", ACS(AC9)<=2%, ACS(AG9)<=5%, AN9>=-15%),\n",
    "#       AND(W9<>\"\", C9<>\"KVI\", ACS(AC9)<=5%),\n",
    "#       AND(C9=\"TAIL\"),\n",
    "#       AND(W9=\"\", C9=\"KVI\", ACS(AG9)<=5%, AN9>=-15%),\n",
    "#       AND(W9=\"\", C9<>\"KVI\")\n",
    "#     ), \"OK\",\n",
    "\n",
    "#     OR(      \n",
    "#       AND(W9<>\"\", ACS(AC9)>5%, ACS(AG9)>5%, AN9<-15%),\n",
    "#       AND(W9=\"\", C9=\"KVI\", ACS(AG9)>5%, AN9<0%),\n",
    "#       AND( C9<>\"TAIL\", AN9<-50%)\n",
    "#     ), \"Alert\",\n",
    "    \n",
    "#     OR(\n",
    "#       AND(W9<>\"\", C9=\"KVI\",OR(ACS(AC9)>2%, ACS(AG9)>5%, AN9>=-100%)),\n",
    "#       AND(W9<>\"\", C9=\"MID\",OR(ACS(AC9)>5%)),\n",
    "#       AND(W9=\"\", C9=\"KVI\")\n",
    "#     ), \"Review\",\n",
    "    \n",
    "#     TRUE, \"Check\"  \n",
    "#   ), \n",
    "#   \"OK\" \n",
    "# )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
