{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Library & INFO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#//LIBRARIES\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import sys\n",
    "import os \n",
    "sys.path.append(os.path.expanduser('~'))\n",
    "\n",
    "from analysts_tools.growth import *\n",
    "\n",
    "#Procurement tools\n",
    "from procurement_lib import send_slack_notification,GoogleSheet,redash\n",
    "from analystcommunity.read_connection_data_warehouse import run_read_dwd_query\n",
    "\n",
    "from datetime import datetime, timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "city = 'SPO'\n",
    "\n",
    "todays_date = datetime.today().strftime('%Y-%m-%d')\n",
    "todays_date = pd.to_datetime(todays_date, format='%Y-%m-%d')\n",
    "todays_date = pd.Timestamp(todays_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sheet=GoogleSheet(\"1P8vmNi-_t7BL0i7jjX2fpfnOuKYqqtrBTmqo3Qr3cfg\")\n",
    "df_scrapper = df_sheet.get_as_dataframe('SCR. ATDO')\n",
    "df_assai_sht = df_sheet.get_as_dataframe('ASSAI')\n",
    "df_fix_prov = df_sheet.get_as_dataframe('FIX INDEX')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Informacion descriptiva de los productos prendidos en PAGINA\n",
    "query = \"\"\"\n",
    "WITH \n",
    "cluster AS (\n",
    "SELECT \n",
    "    sup.source_id,\n",
    "    type AS cluster,\n",
    "    ROW_NUMBER() OVER (PARTITION BY sup.source_id ORDER BY c.last_modified_at DESC, cluster DESC) AS rn\n",
    "FROM dpr_product_pricing.dim_sku_cluster_period c\n",
    "INNER JOIN dpr_shared.dim_stock_unit        su  ON su.sku = c.sku\n",
    "INNER JOIN dpr_shared.dim_stock_unit        sup  ON nvl(nullif(su.source_parent_id,0),su.source_id) = sup.source_id\n",
    "\n",
    "WHERE c.site_id in (4,6,9,11)\n",
    " AND su.active = 1\n",
    " AND su.archived = 0\n",
    " AND su.in_catalog = 1\n",
    " ),\n",
    "\n",
    "skus AS (\n",
    "SELECT\n",
    "    s.identifier_value AS city,\n",
    "        CASE WHEN cat.parent_description = 'Mercearia' \n",
    "         AND cat.description NOT IN ('Arroz', 'Açúcar, adoçantes e doces','Açúcar e adoçantes','Feijão','Grãos','Farinhas e misturas','Azeites, óleos e vinagres')\n",
    "         THEN 'Despensa'\n",
    "        ELSE cat.parent_description \n",
    "    END AS cat,\n",
    "    cat.description AS subcat,\n",
    "    COALESCE(clt.cluster,'TAIL') AS cluster,\n",
    "    sup.source_id,\n",
    "    sup.description\n",
    "FROM dpr_shared.dim_stock_unit          su\n",
    "INNER JOIN dpr_shared.dim_stock_unit    sup ON nvl(nullif(su.source_parent_id,0),su.source_id) = sup.source_id AND su.active = 1 AND su.archived = 0 AND su.in_catalog = 1\n",
    "INNER JOIN dpr_shared.dim_site          s   ON s.site_id = sup.site_id\n",
    "INNER JOIN dpr_shared.dim_category      cat ON cat.category_id = sup.category_id AND cat.super_category = 'Multicategoría'\n",
    "LEFT JOIN cluster                       clt ON clt.source_id = sup.source_id AND clt.rn = 1\n",
    "WHERE city IN ('SPO','CWB','BHZ','VCP')\n",
    "),\n",
    "\n",
    "penetracion AS (\n",
    "SELECT\n",
    "s.identifier_value AS region,\n",
    "COUNT(DISTINCT fs.dim_customer) AS total_custom \n",
    "FROM dpr_sales.fact_sales                   fs\n",
    "INNER JOIN dpr_shared.dim_site              s   ON s.site_id = fs.dim_site\n",
    "INNER JOIN dpr_shared.dim_product           dp  ON dp.product_id = fs.dim_product\n",
    "INNER JOIN dpr_shared.dim_category          cat ON cat.category_id = dp.category_id\n",
    "\n",
    "WHERE \n",
    "    fs.gmv_enabled = TRUE\n",
    "    AND cat.super_category = 'Multicategoría'\n",
    "    AND fulfillment_order_status NOT IN ('CANCELLED', 'ARCHIVED','No value')\n",
    "    AND fs.fb_order_status_id IN (1,6,7,8)\n",
    "    AND fs.is_deleted = FALSE\n",
    "    AND fs.dim_status = 1\n",
    "    AND dp.is_slot = 'false'\n",
    "    AND fs.gmv_pxq_local > 0\n",
    "    AND s.identifier_value IN ('SPO','CWB','VCP','BHZ')\n",
    "    AND DATE(fs.order_submitted_date) >= CURRENT_DATE - 14\n",
    "GROUP BY s.identifier_value\n",
    "),\n",
    "\n",
    "sales AS (\n",
    "SELECT\n",
    "    --DATE(fs.order_submitted_date) AS fecha,\n",
    "    sup.source_id,\n",
    "    100.00*COUNT(DISTINCT fs.dim_customer)::FLOAT/AVG(p.total_custom) AS penet,\n",
    "    SUM(fs.product_quantity_x_step_unit) AS cant,\n",
    "    SUM(fs.gmv_pxq_local)/4.75 AS gmv_usd,\n",
    "    --SUM(COALESCE(fsd.product_discount,0))/4.75 AS dct_usd,\n",
    "    --dct_usd/gmv_usd AS per_dct,\n",
    "    --AVG(COALESCE(inventory_p_fin,cogs_p_mtd)) AS costo,\n",
    "    -- gmv_usd*margin/100.00 AS cash_margin,\n",
    "    -- cash_margin-dct_usd AS net_cash_margin,\n",
    "    -- 100.00*net_cash_margin/gmv_usd AS net_margin,\n",
    "    100.00*gmv_usd/SUM(gmv_usd) OVER (PARTITION BY s.identifier_value) AS gmv_mix--,\n",
    "    -- 100.00*SUM(fs.product_price*fs.product_quantity_x_step_unit)/SUM(min_price*fs.product_quantity_x_step_unit) AS gpi,\n",
    "    -- 100.00*SUM(fs.product_price_discount*fs.product_quantity_x_step_unit)/SUM(min_price*fs.product_quantity_x_step_unit) AS npi\n",
    "\n",
    "    \n",
    "FROM dpr_sales.fact_sales                   fs\n",
    "--INNER JOIN dpr_shared.dim_customer          dc  ON dc.customer_id = fs.dim_customer\n",
    "INNER JOIN dpr_shared.dim_site              s   ON s.site_id = fs.dim_site\n",
    "INNER JOIN dpr_shared.dim_product           dp  ON dp.product_id = fs.dim_product\n",
    "INNER JOIN dpr_shared.dim_category          cat ON cat.category_id = dp.category_id\n",
    "--LEFT JOIN dpr_sales.fact_sales_discounts    fsd ON fs.order_item_id = fsd.order_item_id \n",
    "INNER JOIN dpr_shared.dim_stock_unit        su  ON su.product_id = fs.dim_product\n",
    "INNER JOIN dpr_shared.dim_stock_unit        sup ON nvl(nullif(su.source_parent_id,0),su.source_id) = sup.source_id\n",
    "INNER JOIN penetracion                      p   ON p.region = s.identifier_value\n",
    "--LEFT JOIN dpr_cross_business.fact_cross_business_insights m ON m.dim_stock_unit = sup.stock_unit_id AND m.dim_date = fs.dim_submitted_date AND DATE(fs.order_submitted_date) = DATE(current_date)\n",
    "--LEFT JOIN dpr_product_pricing.obt_benchmark_product_prices  pb  ON pb.stock_unit_id = sup.stock_unit_id AND DATE(fs.order_submitted_date) = pb.benchmark_date\n",
    "\n",
    "WHERE \n",
    "    fs.gmv_enabled = TRUE\n",
    "    AND cat.super_category = 'Multicategoría'\n",
    "    AND fulfillment_order_status NOT IN ('CANCELLED', 'ARCHIVED','No value')\n",
    "    AND fs.fb_order_status_id IN (1,6,7,8)\n",
    "    AND fs.is_deleted = FALSE\n",
    "    AND fs.dim_status = 1\n",
    "    AND dp.is_slot = 'false'\n",
    "    AND fs.gmv_pxq_local > 0\n",
    "    --AND (cogs_p_mtd > 0 OR inventory_p_fin > 0)\n",
    "    AND s.identifier_value IN ('SPO','CWB','VCP','BHZ')\n",
    "    AND DATE(fs.order_submitted_date) >= CURRENT_DATE - 14\n",
    "GROUP BY 1,s.identifier_value\n",
    ")\n",
    "\n",
    "SELECT \n",
    "    skus.*,\n",
    "    penet::FLOAT,\n",
    "    cant::FLOAT,\n",
    "    gmv_usd::FLOAT,\n",
    "    gmv_mix::FLOAT\n",
    "FROM skus\n",
    "LEFT JOIN sales ON skus.source_id = sales.source_id\"\"\"\n",
    "dfq1 = run_read_dwd_query(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Informacion de precios y costos de los productos prendidos en PAGINA\n",
    "query = \"\"\"\n",
    "SELECT \n",
    "sup.source_id,\n",
    "--sup.description,\n",
    "--p.min_base_price,\n",
    "p.min_gross_price::FLOAT AS price,\n",
    "p.min_pricing_price::FLOAT AS net_price,\n",
    "--p.min_sale_price,\n",
    "COALESCE(CASE WHEN dtd_cost_local = 0 THEN inventory_p_fin ELSE dtd_cost_local END,(1-gross_margin/100)*min_gross_price)::FLOAT AS cost,\n",
    "-- ((1-gross_margin/100)*min_gross_price)::FLOAT AS cost,\n",
    "COALESCE(100.00*(1-(cost/price)),gross_margin)::FLOAT AS mg,\n",
    "COALESCE(100.00*(1-(cost/net_price)),net_pricing_margin)::FLOAT AS nmg,\n",
    "discount_pricing_value::FLOAT AS dct\n",
    "\n",
    "FROM dpr_product_pricing.dim_product_current_price p\n",
    "INNER JOIN dpr_shared.dim_stock_unit        su  ON su.stock_unit_id = p.stock_unit_id\n",
    "INNER JOIN dpr_shared.dim_stock_unit        sup ON nvl(nullif(su.source_parent_id,0),su.source_id) = sup.source_id\n",
    "INNER JOIN dpr_shared.dim_category          cat ON cat.category_id = sup.category_id AND cat.super_category = 'Multicategoría'\n",
    "LEFT JOIN  dpr_cross_business.fact_cross_business_insights m ON m.dim_stock_unit = sup.stock_unit_id AND m.dim_date = TO_CHAR(current_date,'YYYYMMDD')::INT\n",
    "LEFT JOIN  dpr_cross_business.int_dtd_cost dt ON dt.dim_stock_unit = sup.stock_unit_id AND dt.dim_date_dtd = TO_CHAR(current_date,'YYYYMMDD')::INT\n",
    "WHERE p.site_id in (4,6,9,11)\n",
    " AND su.active = 1\n",
    " AND su.archived = 0\n",
    " AND su.in_catalog = 1\n",
    " \"\"\"\n",
    "dfq2 = run_read_dwd_query(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Info Offers Dct. Proveedor\n",
    "# query = \"\"\"\n",
    "# SELECT\n",
    "#   s.identifier_value AS city,\n",
    "#   dof.source_id AS offer_id,\n",
    "#   dof.description AS offer_name,\n",
    "#   dot.description AS offer_type,\n",
    "# --   fso.date_created AS date_created,\n",
    "# --   fso.date_updated AS date_updated,\n",
    "# --   duc.\"name\" AS user_creator,\n",
    "# --   duu.name AS user_updater,\n",
    "#   fso.start_date AS start_date,\n",
    "#   fso.end_date AS end_date,\n",
    "#   fso.discount,\n",
    "#   fso.max_uses_per_order,\n",
    "#   fso.max_uses_per_customer,\n",
    "#   fso.use_segment_to_exclude,\n",
    "#   fso.customer_segment_id,\n",
    "#   cat.parent_description AS cat,\n",
    "#   cat.description AS subcat,\n",
    "#   su.source_id,\n",
    "#   su.card_Description AS product\n",
    "    \n",
    "# FROM dpr_sales.fact_sales_offers        fso -- Shared dimensions\n",
    "# INNER JOIN dpr_shared.dim_date              ON dim_date.date_id = fso.dim_date_created\n",
    "# INNER JOIN dpr_shared.dim_site          s   ON s.site_id = fso.dim_site\n",
    "# --INNER JOIN dpr_shared.dim_user_admin duc ON duc.user_admin_id = fso.dim_user_creator\n",
    "# --INNER JOIN dpr_shared.dim_user_admin duu ON duu.user_admin_id = fso.dim_user_updater -- Model dimensions\n",
    "# INNER JOIN dpr_sales.dim_offer          dof ON dof.offer_id = fso.dim_offer\n",
    "# INNER JOIN dpr_sales.dim_offer_type     dot ON dot.offer_type_id = fso.dim_offer_type\n",
    "# INNER JOIN dpr_shared.dim_stock_unit    su  ON fso.dim_stock_unit = su.stock_unit_id\n",
    "# INNER JOIN dpr_shared.dim_category      cat ON su.category_id = cat.category_id\n",
    "# --left join dpr_sales.dim_customers_segments dcs ON dcs.customer_segment_id = fso.customer_segment_id -- Historical changes\n",
    "\n",
    "# WHERE fso.automatically_added = 1\n",
    "#  AND fso.start_date <= current_date\n",
    "#  AND fso.end_date > current_date\n",
    "#  AND dof.description ILIKE 'ACMKTPLC%'\n",
    "#  \"\"\"\n",
    "# df_prov = run_read_dwd_query(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "SELECT\n",
    "--  s.identifier_value AS city,\n",
    "--  dof.source_id AS offer_id,\n",
    "--  dof.description AS offer_name,\n",
    "  dot.description AS offer_type,\n",
    "--   fso.date_created AS date_created,\n",
    "--   fso.date_updated AS date_updated,\n",
    "--   duc.\"name\" AS user_creator,\n",
    "--   duu.name AS user_updater,\n",
    "--   fso.start_date AS start_date,\n",
    "--   fso.end_date AS end_date,\n",
    "  su.source_id AS sku_id,\n",
    "  su.card_Description AS product,\n",
    "  (MAX(fso.discount)/100.00)::FLOAT AS discount\n",
    "--   fso.max_uses_per_order,\n",
    "--   fso.max_uses_per_customer,\n",
    "--   fso.use_segment_to_exclude,\n",
    "--   fso.customer_segment_id,\n",
    "--   cat.parent_description AS cat,\n",
    "--   cat.description AS subcat,\n",
    "\n",
    "    \n",
    "FROM dpr_sales.fact_sales_offers        fso -- Shared dimensions\n",
    "INNER JOIN dpr_shared.dim_date              ON dim_date.date_id = fso.dim_date_created\n",
    "INNER JOIN dpr_shared.dim_site          s   ON s.site_id = fso.dim_site\n",
    "--INNER JOIN dpr_shared.dim_user_admin duc ON duc.user_admin_id = fso.dim_user_creator\n",
    "--INNER JOIN dpr_shared.dim_user_admin duu ON duu.user_admin_id = fso.dim_user_updater -- Model dimensions\n",
    "INNER JOIN dpr_sales.dim_offer          dof ON dof.offer_id = fso.dim_offer\n",
    "INNER JOIN dpr_sales.dim_offer_type     dot ON dot.offer_type_id = fso.dim_offer_type\n",
    "INNER JOIN dpr_shared.dim_stock_unit    su  ON fso.dim_stock_unit = su.stock_unit_id\n",
    "INNER JOIN dpr_shared.dim_category      cat ON su.category_id = cat.category_id\n",
    "--left join dpr_sales.dim_customers_segments dcs ON dcs.customer_segment_id = fso.customer_segment_id -- Historical changes\n",
    "\n",
    "WHERE fso.automatically_added = 1\n",
    " AND s.identifier_value = 'SPO'\n",
    " AND fso.start_date <= current_date\n",
    " AND fso.end_date > current_date\n",
    " AND offer_type = 'Hook'\n",
    "GROUP BY 1,2,3\n",
    "\"\"\"\n",
    "df_hook = run_read_dwd_query(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "with rango_de_fechas as (\n",
    "select TO_CHAR(DATE_TRUNC('week', DATEADD(week, -8, CURRENT_DATE)),'YYYYMMDD')::int as dim_start_date\n",
    "    , to_char(current_date,'YYYYMMDD')::int as dim_end_date\n",
    "    , to_char(to_date(dim_start_date,'YYYYMMDD') - 9,'YYYYMMDD')::int as start_dim_date_bech\n",
    ")\n",
    "\n",
    ",sales_diario as (\n",
    "select su.sku                                                           as sku\n",
    "    , su.stock_unit_id                                                  as dim_stock_unit \n",
    "    , close.year_number                                                 as year\n",
    "    , close.year_week_number                                            as week\n",
    "    , close.year_week_char                                              as semana\n",
    "    , 'SPO'                                                             as city\n",
    "    , cat.description                                                   as sub_category\n",
    "    , cat.parent_description                                            as category \n",
    "    , su.description                                                    as description\n",
    "    , su.source_id                                                      as product_id\n",
    "    , joa.proveedor_split\n",
    "    , sum(s.gmv_local)                                                  as gmv_local_sku\n",
    "    , sum(sum(s.gmv_local)) over (partition by close.year_week_char)    as gmv_local_multi\n",
    "    , sum(s.product_price_discount * s.product_quantity_x_step_unit) / nullif(sum(s.product_quantity_x_step_unit),0) as net_price\n",
    "    , close.date_id\n",
    "    , sum(s.product_quantity_x_step_unit)                                                   as product_quantity\n",
    "    , sum(s.product_price * s.product_quantity_x_step_unit) / nullif(sum(s.product_quantity_x_step_unit),0) as gross_price\n",
    "    , close.full_date\n",
    "from dpr_sales.fact_sales                   s\n",
    "    inner join rango_de_fechas              dt      on s.dim_submitted_date between dt.dim_start_date and dt.dim_end_date\n",
    "    inner join dpr_shared.dim_stock_unit    su      on su.product_id    = s.dim_product\n",
    "    inner join dpr_shared.dim_category      cat     on cat.category_id  = su.category_id\n",
    "    inner join dpr_shared.dim_date          close   on close.date_id    = s.dim_submitted_date\n",
    "    left join (select distinct product_id,\n",
    "                  rtrim(split_part(provider, '(', 1)) proveedor_split\n",
    "                from mvp_multicategory.raw_prc_mktplc_product_provider\n",
    "                where true\n",
    "                  and region_code = 'SPO'\n",
    "                  and upload_date = (select max(upload_date) from mvp_multicategory.raw_prc_mktplc_product_provider)) joa on joa.product_id=su.source_id\n",
    "                                            \n",
    "where true\n",
    "    and s.gmv_enabled\n",
    "    and cat.super_category = 'Multicategoría'\n",
    "    and s.dim_site=4\n",
    "group by su.sku\n",
    "    , su.stock_unit_id\n",
    "    , close.year_week_char\n",
    "    , cat.description\n",
    "    , cat.parent_description\n",
    "    , su.description\n",
    "    , su.source_id\n",
    "    , close.year_week_number\n",
    "    , close.year_number\n",
    "    , close.date_id\n",
    "    , joa.proveedor_split\n",
    "    , close.full_date)\n",
    "    \n",
    ", purchase_ponderado_diario AS ( \n",
    "    SELECT max(p.price_local) price_day\n",
    "        , sum (p.quantity) qty_day\n",
    "        , fecha.date_id\n",
    "        , p.dim_stock_unit\n",
    "        , fecha.year_number             as year\n",
    "        , fecha.year_week_number        as week\n",
    "        , fecha.year_week_char\n",
    "    FROM dpr_sourcing.fact_purchases        p  \n",
    "    inner join rango_de_fechas              dt      on p.dim_order_delivery_date between dt.dim_start_date and dt.dim_end_date\n",
    "    inner join dpr_shared.dim_date          fecha   on fecha.date_id=p.dim_order_delivery_date\n",
    "    inner join dpr_shared.dim_stock_unit    su      ON p.dim_stock_unit = su.stock_unit_id\n",
    "    inner join dpr_sourcing.dim_supplier    ds      ON ds.supplier_id = p.dim_supplier\n",
    "    inner join dpr_shared.dim_category      dc      ON dc.category_id = su.category_id\n",
    "    WHERE TRUE\n",
    "        AND p.dim_order_deleted_date = 0\n",
    "        AND p.order_received\n",
    "        AND p.dim_site = 4\n",
    "        -- and su.sku IN ('SPO-BJP3304-CAT107094-188123:452076:452075:232294','SPO-FRU1-CAT120730-302313:759014:759013:397303','SPO-FRU1-CAT106987-123461:282402:282401:153095')\n",
    "    GROUP BY fecha.date_id,p.dim_stock_unit,fecha.year_number,fecha.year_week_number, fecha.year_week_char\n",
    ")\n",
    "\n",
    ",pps_semanal as (\n",
    "    select pps.year_week_char semana,\n",
    "        pps.year,\n",
    "        pps.week,\n",
    "        pps.dim_stock_unit,\n",
    "        sum(pps.price_day * pps.qty_day) / sum(pps.qty_day) as avg_ponderado_ocs\n",
    "    from purchase_ponderado_diario pps\n",
    "    group by pps.year_week_char,\n",
    "        pps.year,\n",
    "        pps.week,\n",
    "        pps.dim_stock_unit\n",
    ")\n",
    "    \n",
    ", precio_diario as (\n",
    "select dtd.dim_stock_unit                                     as dim_stock_unit\n",
    "    ,(CASE WHEN dtd.dtd_cost_local = 0 THEN m.inventory_p_fin ELSE dtd.dtd_cost_local END) as price\n",
    "    ,fecha.date_id\n",
    "from dpr_cross_business.int_dtd_cost                                dtd\n",
    "    inner join rango_de_fechas                                      dt      on dtd.dim_date_dtd between dt.dim_start_date and dt.dim_end_date\n",
    "    LEFT JOIN  dpr_cross_business.fact_cross_business_insights      m       ON m.dim_stock_unit = dtd.dim_stock_unit AND m.dim_date = dtd.dim_date_dtd\n",
    "    inner join dpr_shared.dim_date                                  fecha   on (fecha.date_id = dtd.dim_date_dtd)\n",
    "    inner join dpr_shared.dim_stock_unit                            p       on (p.stock_unit_id = dtd.dim_stock_unit)\n",
    "    inner join dpr_shared.dim_category                              cat     on (p.category_id=cat.category_id)\n",
    "    -- left join sales                                                         on (sales.dim_stock_unit = dtd.dim_stock_unit and sales.year = fecha.year_number and sales.week=fecha.year_week_number)\n",
    "where true\n",
    "  and cat.super_category = 'Multicategoría'\n",
    "  and p.site_id = 4 \n",
    "  and fecha.full_date >= DATE(current_Date) - 1\n",
    "--   and p.sku in ('SPO-BJP3304-CAT107094-188123:452076:452075:232294','SPO-FRU1-CAT120730-302313:759014:759013:397303','SPO-FRU1-CAT106987-123461:282402:282401:153095')\n",
    "group by dtd.dim_stock_unit,price,fecha.date_id\n",
    ")\n",
    "\n",
    ", ranked_data AS ( --QUERY BASE PRECIOS PARA BECH\n",
    "    SELECT\n",
    "        competitor.competitor_name AS competitor_name,   \n",
    "        quotation_date.full_date AS quotation_date,\n",
    "        quotation_date.date_id,\n",
    "        ROUND(cpp.product_selected_price, 2)::float AS price,\n",
    "        CASE\n",
    "            WHEN (competitor.competitor_name ILIKE '%atacadao%' OR competitor.competitor_name ILIKE '%atacadão%') THEN '01 MED_atacadao'\n",
    "            WHEN competitor.competitor_name ILIKE '%assaí%' THEN '02 MED_assai'\n",
    "            ELSE '03 MED_ALL'\n",
    "        END AS competitor_group,\n",
    "        LEAD(quotation_date.full_date, 1) OVER (PARTITION BY competitor.competitor_name, cpp.dim_stock_unit ORDER BY cpp.quotation_date,cpp.product_selected_price) AS next_quotation_date\n",
    "        , su.stock_unit_id as dim_stock_unit\n",
    "    FROM dpr_product_pricing.fact_collected_product_prices cpp\n",
    "    inner join rango_de_fechas  dt on cpp.dim_quotation_date between dt.start_dim_date_bech and dt.dim_end_date\n",
    "    INNER JOIN dpr_shared.dim_date quotation_date ON cpp.dim_quotation_date = quotation_date.date_id\n",
    "    INNER JOIN dpr_shared.dim_site site ON cpp.dim_site = site.site_id\n",
    "    INNER JOIN dpr_shared.dim_category cat ON cpp.dim_category = cat.category_id\n",
    "    INNER JOIN dpr_product_pricing.dim_product_competitor competitor ON cpp.dim_competitor = competitor.competitor_id\n",
    "    INNER JOIN dpr_shared.dim_stock_unit su ON cpp.dim_stock_unit = su.stock_unit_id\n",
    "    inner join dpr_product_pricing.dim_product_source_type source_type  on cpp.dim_source_type = source_type.source_type_id\n",
    "    inner join dpr_product_pricing.dim_product_outlier_type outlier_type on cpp.dim_outlier_type = outlier_type.outlier_type_id\n",
    "    -- aca poner outlier\n",
    "  WHERE true\n",
    "    AND site.site_id=4\n",
    "    and outlier_type.description not in ('Manual Outlier')\n",
    "    and source_type.description IN ('Zukkin','Scrapers')\n",
    "    and cpp.product_selected_price >0\n",
    "    and cpp.super_category = 'Multicategoría'\n",
    "    -- and su.sku in ('SPO-BJP3304-CAT107094-188123:452076:452075:232294','SPO-FRU1-CAT120730-302313:759014:759013:397303','SPO-FRU1-CAT106987-123461:282402:282401:153095')\n",
    ")\n",
    ", expanded_dates AS ( \n",
    "    -- Aquí calculamos la fecha final de vigencia del precio (maximo 8 dias despues)\n",
    "    SELECT\n",
    "        competitor_name,\n",
    "        dim_stock_unit,\n",
    "        price,\n",
    "        quotation_date,\n",
    "        competitor_group,\n",
    "        LEAST(\n",
    "            -- Calculamos la fecha hasta donde se debería extender la vigencia\n",
    "            next_quotation_date - INTERVAL '1 day',\n",
    "            quotation_date + INTERVAL '8 days',\n",
    "            CURRENT_DATE + 8\n",
    "        ) AS valid_until\n",
    "    FROM ranked_data\n",
    ")\n",
    "\n",
    ", median_bench_diario as (\n",
    "  --granularidad competitor_group,sku,semana  MEDIAN(q.price) \n",
    "    SELECT q.competitor_group,\n",
    "        q.quotation_date,\n",
    "        q.date_id,\n",
    "        q.dim_stock_unit,\n",
    "        q.data_points as data_points,\n",
    "        MEDIAN(q.price) AS bech_day,\n",
    "        ROW_NUMBER() OVER (PARTITION BY q.date_id, q.dim_stock_unit ORDER BY \n",
    "            CASE \n",
    "                WHEN q.competitor_group = '01 MED_atacadao' THEN 1\n",
    "                WHEN q.competitor_group = '02 MED_assai' THEN 2\n",
    "                ELSE 3\n",
    "            END) AS competitor_priority\n",
    "    FROM (   --granularidad competitor_name,dia,sku\n",
    "        SELECT \n",
    "            ed.competitor_name,\n",
    "            ed.price,\n",
    "            dd.full_date AS quotation_date,\n",
    "            dd.date_id , \n",
    "            ed.dim_stock_unit ,\n",
    "            ed.competitor_group,\n",
    "            CASE WHEN dd.full_date = ed.quotation_date THEN true ELSE false END AS is_original,\n",
    "            count(1) over (partition by dd.full_date,ed.competitor_group,ed.dim_stock_unit) data_points\n",
    "        FROM expanded_dates ed\n",
    "        JOIN dpr_shared.dim_date dd\n",
    "            ON dd.full_date BETWEEN ed.quotation_date AND ed.valid_until\n",
    "        WHERE dd.full_date <= CURRENT_DATE \n",
    "    ) q \n",
    "    GROUP BY q.competitor_group,\n",
    "        q.quotation_date,\n",
    "        q.date_id,\n",
    "        q.dim_stock_unit,\n",
    "        q.data_points\n",
    "),\n",
    "\n",
    "pre_info AS (  \n",
    "select ss.full_date\n",
    "    , ss.product_id\n",
    "    , ss.sku\n",
    "    , ss.description\n",
    "    , ss.category\n",
    "    , ss.sub_category\n",
    "    , ss.proveedor_split\n",
    "    , sum(ss.gmv_local_sku*1.0) as gmv_local_sku\n",
    "    , 100.0 * sum(ss.gmv_local_sku) / sum(sum(ss.gmv_local_sku)) over (partition by ss.full_date, ss.proveedor_split) as \"% del GMV del proveedor\"\n",
    "    -- , COALESCE(pps_semanal.avg_ponderado_ocs, LAG(pps_semanal.avg_ponderado_ocs IGNORE NULLS) OVER (PARTITION BY ss.sku ORDER BY ss.semana)) as avg_ponderado_ocs\n",
    "    , sum(ps.price * ss.gmv_local_sku) / sum(ss.gmv_local_sku) as ponderado_invcost_dtd\n",
    "    , sum(mps.bech_day * ss.gmv_local_sku) / sum(ss.gmv_local_sku) as ponderado_betch\n",
    "    , (sum(ss.net_price * (case when mps.bech_day is null then 0 else ss.gmv_local_sku end)) / \n",
    "      sum(mps.bech_day * (case when mps.bech_day is null then 0 else ss.gmv_local_sku end)))*100.0 as net_price_index\n",
    "    , case when 100.0 * (1 - (sum(ps.price * ss.gmv_local_sku) / sum(mps.bech_day * ss.gmv_local_sku))) <= -10 then 'Not Acceptable'\n",
    "           when 100.0 * (1 - (sum(ps.price * ss.gmv_local_sku) / sum(mps.bech_day * ss.gmv_local_sku))) <= 0 then 'Very Bad'\n",
    "           when 100.0 * (1 - (sum(ps.price * ss.gmv_local_sku) / sum(mps.bech_day * ss.gmv_local_sku))) <= 10 then 'Bad'\n",
    "           when 100.0 * (1 - (sum(ps.price * ss.gmv_local_sku) / sum(mps.bech_day * ss.gmv_local_sku))) <= 20 then 'Good'\n",
    "           else 'Very Good' end as calificacion\n",
    "    , avg(mps.data_points*1.0) as avg_data_points\n",
    "    , 100.0 * (1 - (sum(ps.price * ss.gmv_local_sku) / sum(mps.bech_day * ss.gmv_local_sku))) as compete_margen\n",
    "    , ss.gmv_local_multi\n",
    "    -- , COALESCE(pps_semanal.semana,   LAG(pps_semanal.semana   IGNORE NULLS) OVER (PARTITION BY ss.sku ORDER BY ss.semana)) AS semana_ponderacion_ocs\n",
    "    , null as \"Calificación Infoprice (Revisar con Rich)\"\n",
    "    , null as \"Elasticidad (Cami Hurtado) \"\n",
    "from sales_diario ss  \n",
    "    left join precio_diario                     ps  on ss.dim_stock_unit = ps.dim_stock_unit            and ss.date_id = ps.date_id\n",
    "    left join median_bench_diario               mps on ss.dim_stock_unit = mps.dim_stock_unit           and ss.date_id = mps.date_id\n",
    "    left join purchase_ponderado_diario         pps on ss.dim_stock_unit = pps.dim_stock_unit           and ss.date_id = pps.date_id\n",
    "    left join pps_semanal                           on ss.dim_stock_unit = pps_semanal.dim_stock_unit   and ss.year    = pps_semanal.year and ss.week = pps_semanal.week\n",
    "where true\n",
    "    and ps.price > 0\n",
    "    and mps.bech_day > 0\n",
    "    and mps.competitor_priority = 1\n",
    "    and ss.full_date >= DATE(CURRENT_DATE) - 1\n",
    "group by ss.full_date\n",
    "    , ss.product_id\n",
    "    , ss.sku\n",
    "    , ss.description\n",
    "    , ss.category\n",
    "    , ss.sub_category\n",
    "    , ss.proveedor_split\n",
    "    , ss.gmv_local_multi\n",
    "    , pps_semanal.avg_ponderado_ocs\n",
    "    , pps_semanal.semana\n",
    ")\n",
    "\n",
    "\n",
    "SELECT \n",
    "full_date,\n",
    "product_id AS source_id,\n",
    "compete_margen\n",
    "\n",
    "FROM (\n",
    "    SELECT \n",
    "    full_date,\n",
    "    product_id,\n",
    "    compete_margen,\n",
    "    ROW_NUMBER() OVER (\n",
    "            PARTITION BY product_id\n",
    "            ORDER BY full_date DESC\n",
    "        ) AS row_num\n",
    "    FROM pre_info\n",
    ") AS a\n",
    "WHERE row_num = 1\n",
    "\"\"\"\n",
    "df_compete = run_read_dwd_query(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "select\n",
    "    competitor.competitor_name AS competitor_name,   \n",
    "    site.identifier_value as site_code,\n",
    "    quotation_date.full_date AS quotation_date,\n",
    "    su.source_id,\n",
    "    ROUND(cpp.product_selected_price,2)::float as price--ROUND(MEDIAN(cpp.product_selected_price),2)::float as price\n",
    "from dpr_product_pricing.fact_collected_product_prices cpp\n",
    "    inner join dpr_shared.dim_date quotation_date\n",
    "        on cpp.dim_quotation_date = quotation_date.date_id\n",
    "    inner join dpr_shared.dim_time quotation_time\n",
    "        on cpp.dim_quotation_time = quotation_time.time_id\n",
    "    inner join dpr_shared.dim_site site\n",
    "        on cpp.dim_site = site.site_id\n",
    "    inner join dpr_shared.dim_category cat\n",
    "        on cpp.dim_category = cat.category_id\n",
    "    inner join dpr_product_pricing.dim_product_outlier_type outlier_type\n",
    "        on cpp.dim_outlier_type = outlier_type.outlier_type_id\n",
    "    inner join dpr_product_pricing.dim_product_source_type source_type\n",
    "        on cpp.dim_source_type = source_type.source_type_id\n",
    "    inner join dpr_product_pricing.dim_product_competitor competitor\n",
    "        on cpp.dim_competitor = competitor.competitor_id\n",
    "    inner join dpr_product_pricing.dim_product_competitor_type competitor_type\n",
    "        on(\n",
    "            case\n",
    "                when cpp.super_category = 'Fruver'\n",
    "                    then competitor.product_competitor_type_id_fruver = competitor_type.competitor_type_id\n",
    "                when cpp.super_category = 'Multicategoría'\n",
    "                    then competitor.product_competitor_type_id_multicategoria = competitor_type.competitor_type_id\n",
    "            end\n",
    "        )\n",
    "    inner join dpr_shared.dim_stock_unit su\n",
    "        on cpp.dim_stock_unit = su.stock_unit_id\n",
    "where quotation_date.full_date >= current_date - 10\n",
    "    AND outlier_type.description not IN ('Manual Outlier')\n",
    "    AND cpp.collected_product_prices_id <> 12883150\n",
    "    AND source_type.description IN ('Zukkin','Scrapers')\n",
    "    AND (\n",
    "        competitor.competitor_name NOT ILIKE '%cayena%'\n",
    "        AND (\n",
    "            competitor.competitor_name <> 'Atacadao_V2'\n",
    "            OR su.source_id IN {skus_scrapper}\n",
    "        )\n",
    "    )\n",
    "    AND site.identifier_value IN ('SPO')\n",
    "\n",
    "--GROUP BY 1,2,3,4\n",
    "\"\"\".format(skus_scrapper = tuple(list(df_scrapper.source_id.unique()) + [1]))\n",
    "df_zkkkkk = run_read_dwd_query(query)\n",
    "\n",
    "df_zkkkkk = df_zkkkkk.dropna().reset_index(drop=True)\n",
    "df_zkkkkk['lifetime'] = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure dataframe is sorted by 'quotation_date'\n",
    "df_zkkkkk = df_zkkkkk.sort_values(by='quotation_date')\n",
    "\n",
    "# Generate the required rows for missing dates\n",
    "new_rows = []\n",
    "\n",
    "for (competitor, source_id), group in df_zkkkkk.groupby(['competitor_name', 'source_id']):\n",
    "    group = group.sort_values(by='quotation_date')\n",
    "    last_known_price = None\n",
    "    last_known_date = None\n",
    "    lifetime = 8\n",
    "    \n",
    "    for current_index in range(len(group)):\n",
    "        current_date = group.iloc[current_index]['quotation_date']\n",
    "        price = group.iloc[current_index]['price']\n",
    "        \n",
    "        # If this is not the first iteration, fill in missing dates\n",
    "        if last_known_date is not None:\n",
    "            days_diff = (current_date - last_known_date).days\n",
    "            if days_diff > 1:\n",
    "                for j in range(1, min(days_diff, lifetime + 1)):\n",
    "                    new_date = last_known_date + timedelta(days=j)\n",
    "                    new_row = {\n",
    "                        'site_code': group.iloc[current_index]['site_code'],\n",
    "                        'quotation_date': new_date,\n",
    "                        'competitor_name': competitor,\n",
    "                        'source_id': source_id,\n",
    "                        'price': last_known_price,\n",
    "                        'lifetime': lifetime - j\n",
    "                    }\n",
    "                    new_rows.append(new_row)\n",
    "                    \n",
    "                    # Stop if we reach a new datapoint date\n",
    "                    if new_date + timedelta(days=1) == current_date:\n",
    "                        break\n",
    "        \n",
    "        # Update the last known values and reset lifetime\n",
    "        last_known_price = price\n",
    "        last_known_date = current_date\n",
    "        lifetime = 8  # Reset lifetime\n",
    "\n",
    "    # After processing all known dates for the group, continue generating rows until lifetime reaches 0\n",
    "    while lifetime > 0:\n",
    "        last_known_date += timedelta(days=1)\n",
    "        new_row = {\n",
    "            'site_code': group.iloc[-1]['site_code'],\n",
    "            'quotation_date': last_known_date,\n",
    "            'competitor_name': competitor,\n",
    "            'source_id': source_id,\n",
    "            'price': last_known_price,\n",
    "            'lifetime': lifetime - 1\n",
    "        }\n",
    "        new_rows.append(new_row)\n",
    "        lifetime -= 1\n",
    "\n",
    "# Append new rows to the dataframe\n",
    "df_zkkkkk = df_zkkkkk.append(new_rows, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.merge(dfq1.loc[dfq1.city == city],dfq2,left_on=['source_id'],right_on=['source_id'],how='inner')\n",
    "df = df.sort_values(by=['cat','subcat', 'cluster','gmv_mix'], ascending=[False, True, True,False]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bench = df_zkkkkk.loc[df_zkkkkk.quotation_date == todays_date.date()].reset_index(drop=True).copy()\n",
    "\n",
    "# Function to calculate the required statistics\n",
    "def calculate_statistics(df):\n",
    "    return df.groupby('source_id')['price'].agg(\n",
    "        num_data_points='count',\n",
    "        num_competitors=lambda x: df.loc[x.index, 'competitor_name'].nunique(),\n",
    "        min_price='min',\n",
    "        avg_price='mean',\n",
    "        median_price='median',\n",
    "        max_price='max',\n",
    "        #delta_min_max=lambda x: x.max() - x.min()\n",
    "    ).reset_index()\n",
    "\n",
    "# Calculate statistics for all competitors\n",
    "stats_all = calculate_statistics(df_bench)\n",
    "stats_all.columns = ['source_id', 'POINTS Med ALL', 'num_competitors_all', 'min_price_all', 'avg_price_all', 'Med ALL', 'max_price_all']\n",
    "\n",
    "# Filter for competitors that include \"assai\" in their name and calculate statistics\n",
    "df_assai = df_bench[df_bench['competitor_name'].str.contains('assaí', case=False, na=False)]\n",
    "stats_assai = calculate_statistics(df_assai)\n",
    "stats_assai.columns = ['source_id', 'POINTS Med Assai', 'num_competitors_assai', 'min_price_assai', 'avg_price_assai', 'Med Assai', 'max_price_assai']\n",
    "\n",
    "# Filter for competitors that include \"atacadao\" or \"atacadão\" in their name and calculate statistics\n",
    "df_atacadao = df_bench[df_bench['competitor_name'].str.match(r'(?i)^atacad[aã]o') & ~df_bench['competitor_name'].str.contains(r'(?i)^Atacadao_V2$')]\n",
    "stats_atacadao = calculate_statistics(df_atacadao)\n",
    "stats_atacadao.columns = ['source_id', 'POINTS Med Atacadao', 'num_competitors_atacadao', 'min_price_atacadao', 'avg_price_atacadao', 'Med Atacadao', 'max_price_atacadao']\n",
    "\n",
    "# Filter for competitors that include \"atacadao_v2\" the scrapper\n",
    "df_scrapper_atacadao = df_bench[df_bench['competitor_name'].str.contains(r'(?i)^Atacadao_V2$')]\n",
    "stats_scrapper_atacadao = calculate_statistics(df_scrapper_atacadao)\n",
    "stats_scrapper_atacadao.columns = ['source_id', 'POINTS Scrp. Atacadao', 'num_competitors_atacadao_scrapper', 'Scrp. Atacadao', 'avg_price_atacadao_scrapper', 'Med atacadao_scrapper', 'max_price_atacadao_scrapper']\n",
    "\n",
    "# Merge the results\n",
    "bench_df = stats_all.merge(stats_assai, on='source_id', how='left').merge(stats_atacadao, on='source_id', how='left').merge(stats_scrapper_atacadao, on='source_id', how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PRINTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PRINT DESCRIPTIVE INFO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the manual info in the file\n",
    "df_info_sheet = df_sheet.get_as_dataframe('info_to_py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of columns to be divided by 100\n",
    "columns_to_divide = ['penet', 'gmv_mix', 'mg', 'nmg', 'dct']\n",
    "\n",
    "# Divide the specified columns by 100\n",
    "df_print = df.copy()\n",
    "df_print[columns_to_divide] = df_print[columns_to_divide] / 100\n",
    "\n",
    "# Se pegan los parametros de la estrategia establecida para mantenerse constante\n",
    "df_print = pd.merge(df_print,df_info_sheet,left_on=['source_id'],right_on=['ID'],how='left')\n",
    "df_print.drop(columns=['ID'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PEGAR INFO\n",
    "df_sheet.set_with_dataframe(df_print, worksheet='info', clear=True, autocreate=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PRINT BENCH INFO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aca dejamos las primeras 19 columnas porque no queremos incluir las columnas del SCRAPPER de Atacadao"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PEGAR INFO\n",
    "df_sheet.set_with_dataframe(bench_df.iloc[:, :19], worksheet='data', clear=True, autocreate=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PRINT MIN BENCH INFO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by source_id and get the row with the minimum price\n",
    "min_price_idx = df_bench.groupby('source_id')['price'].idxmin()\n",
    "\n",
    "# Use these indices to get the rows with the minimum price\n",
    "df_bench_min = df_bench.loc[min_price_idx, ['source_id', 'competitor_name', 'price']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PEGAR INFO\n",
    "df_sheet.set_with_dataframe(df_bench_min, worksheet='min_bench', clear=True, autocreate=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## APPLYING RULES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### APLICAR ESTRATEGIA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1. NEW OR NULL VALUES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paso 1: Encontrar las filas donde 'Strategy' es nulo\n",
    "na_rows = df_print[df_print['Strategy'].isna()]\n",
    "\n",
    "# Paso 2: Reemplazar valores nulos con valores predeterminados\n",
    "df_print.loc[na_rows.index, 'Tipo producto'] = df_print.loc[na_rows.index, 'Tipo producto'].fillna('Mais barato')\n",
    "df_print.loc[na_rows.index, 'Index'] = df_print.loc[na_rows.index, 'Index'].fillna(1)\n",
    "df_print.loc[na_rows.index, 'Frecuency'] = df_print.loc[na_rows.index, 'Frecuency'].fillna('Diaria')\n",
    "df_print.loc[na_rows.index, 'Strategy'] = df_print.loc[na_rows.index, 'Strategy'].fillna('Med Atacadao')\n",
    "\n",
    "# Paso 3: Calcular el promedio de 'Min margin' para cada subcategoría y usarlo para llenar los valores nulos en 'Min margin'\n",
    "avg_margin_by_subcat = df_print.groupby('subcat')['Min margin'].transform('mean')\n",
    "df_print.loc[na_rows.index, 'Min margin'] = df_print.loc[na_rows.index, 'Min margin'].fillna(avg_margin_by_subcat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stg = df_print.loc[~df_print['Strategy'].isna()].reset_index(drop=True).copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.1 Check index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the condition as df\n",
    "df_filtro = df_sheet.get_as_dataframe('strategy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_stg = df_stg.merge(df_compete[['source_id','compete_margen']], on='source_id', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hacer un merge entre df_stg y df_compete para obtener el 'compete' asociado a cada 'id'\n",
    "df_stg = df_stg.merge(df_compete[['source_id','compete_margen']], on='source_id', how='left')\n",
    "\n",
    "# Función para obtener el 'Index' basado en el valor de 'compete'\n",
    "def get_index(compete_value):\n",
    "    if pd.isna(compete_value):  # Si el valor de 'compete' es nulo, asignar 1.00 (100%)\n",
    "        return 1.00\n",
    "    else:\n",
    "        # Encontrar el índice correspondiente tomando el valor más cercano hacia abajo\n",
    "        filtro = df_filtro[df_filtro['Compete'] <= compete_value].sort_values(by='Compete', ascending=False).head(1)\n",
    "        if filtro.empty:\n",
    "            return 1.00  # Si no se encuentra un match, retornar 1.00\n",
    "        return filtro['Index'].values[0]\n",
    "\n",
    "# Aplicar la función para definir la columna 'Index' en df_stg\n",
    "df_stg['Index'] = df_stg['compete_margen'].apply(get_index)\n",
    "\n",
    "# Eliminar la columna 'compete' que se agregó en el merge\n",
    "df_stg = df_stg.drop(columns=['compete_margen'])\n",
    "\n",
    "# Mostrar el resultado final\n",
    "#print(df_stg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the condition as df\n",
    "df_top = df_sheet.get_as_dataframe('top_mind')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convertir los source_id de df_top a un set para una búsqueda más rápida\n",
    "source_ids_top = set(df_top['source_id'])\n",
    "\n",
    "# Actualizar la columna 'Index' a 1.00 (100%) si el source_id está en df_top\n",
    "df_stg.loc[df_stg['source_id'].isin(source_ids_top), 'Index'] = 1.00"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5.1 Add Indexes Dct. Proveedor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For update Index\n",
    "df_fix_prov_2 = df_fix_prov.loc[(df_fix_prov.fecha_inicio <= str(todays_date.date())) & (df_fix_prov.fecha_fin > str(todays_date.date()))]# & (df_fix_prov.source_id.isin(df_prov.source_id.unique()))]\n",
    "# For turn back Index\n",
    "df_fix_prov_3 = df_fix_prov.loc[((pd.to_datetime(df_fix_prov.fecha_fin)) == todays_date + timedelta(days=-1))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary from df_fix in order to get back the previous Index !!\n",
    "fix_index_dict_back = df_fix_prov_3.set_index('source_id')['INDEX ACTUAL'].to_dict()\n",
    "\n",
    "# Apply the updates\n",
    "mask_prov_back = df_stg['source_id'].isin(fix_index_dict_back.keys())\n",
    "df_stg.loc[mask_prov_back, 'Index'] = df_stg.loc[mask_prov_back, 'source_id'].map(fix_index_dict_back)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary from df_fix in order to get to the desired Index !!\n",
    "fix_index_dict = df_fix_prov_2.set_index('source_id')['INDEX FIJO'].to_dict()\n",
    "\n",
    "# Apply the updates\n",
    "mask_prov = df_stg['source_id'].isin(fix_index_dict.keys())\n",
    "df_stg.loc[mask_prov, 'Index'] = df_stg.loc[mask_prov, 'source_id'].map(fix_index_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5.2 Add Indexes Hooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary from df_fix to get back the previous Index !!\n",
    "fix_index_hook = df_hook.set_index('sku_id')['discount'].to_dict()\n",
    "\n",
    "# Apply the updates\n",
    "mask_index_hook = df_stg['source_id'].isin(fix_index_hook.keys())\n",
    "\n",
    "# Subtract the discount from the Index\n",
    "df_stg.loc[mask_index_hook, 'Index'] = df_stg.loc[mask_index_hook, 'Index'] - df_stg.loc[mask_index_hook, 'source_id'].map(fix_index_hook)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Logic to define the price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def formula(row):\n",
    "    # Obtener la estrategia y el source_id\n",
    "    primary_strategy = row['Strategy']\n",
    "    source_id = row['source_id']\n",
    "    index = row['Index']\n",
    "\n",
    "    # Filtrar bench_df para el source_id específico\n",
    "    filtered_bench = bench_df[bench_df['source_id'] == source_id]\n",
    "    \n",
    "    # Inicializar bench_value\n",
    "    bench_value = None\n",
    "    \n",
    "    # Estrategias en orden de preferencia\n",
    "    strategies = ['Scrp. Atacadao','Med Atacadao', 'Med Assai', 'Med ALL']\n",
    "    \n",
    "    # Encontrar el índice de la estrategia primaria\n",
    "    if source_id in df_assai_sht.source_id.unique():\n",
    "        primary_index = 2\n",
    "    else:\n",
    "        primary_index = 0#strategies.index(primary_strategy)\n",
    "    \n",
    "    \n",
    "\n",
    "    # Verificar la estrategia primaria y las siguientes en el orden de preferencia\n",
    "    for strategy in strategies[primary_index:]:\n",
    "        if not filtered_bench.empty and pd.notna(filtered_bench[strategy].values[0]):\n",
    "            if (filtered_bench[f'POINTS {strategy}'].values[0] > 1):\n",
    "                bench_value = filtered_bench[strategy].values[0]\n",
    "                break\n",
    "            elif strategy == 'Med ALL':\n",
    "                bench_value = filtered_bench[strategy].values[0]\n",
    "                break\n",
    "    \n",
    "    # Si no se encontró un valor válido en las estrategias, usar el valor fallback\n",
    "    if bench_value is None:\n",
    "        if row['mg'] > row['Min margin']:\n",
    "            fallback_value = row['net_price']\n",
    "        else:\n",
    "            fallback_value = round((row['cost'] / (1 - row['Min margin']))*(1-row['dct']), 2) #incluimos dct\n",
    "        return 'Margin', None, fallback_value, 'NO BENCH'\n",
    "    \n",
    "\n",
    "    # New Bench Values Based On Index\n",
    "    bench_value = bench_value*index #Aca multiplica por el Index\n",
    "    # Calcular el nuevo margen\n",
    "    new_margin = 1 - (row['cost'] / (bench_value*(1+row['dct']))) #new margin gross\n",
    "    \n",
    "    # Comprobar si el nuevo margen es mayor que el margen mínimo\n",
    "    if new_margin > row['Min margin']:\n",
    "        # Si es así, retornar el valor de referencia como el nuevo precio y el bench_value como new_bench\n",
    "        return strategy, round(bench_value, 2), round(bench_value, 2), 'PRICED BENCH'\n",
    "    else:\n",
    "        # De lo contrario, retornar el valor fallback y el bench_value como new_bench\n",
    "        fallback_value = round((row['cost'] / (1 - row['Min margin']))*(1-row['dct']), 2)\n",
    "        return strategy, round(bench_value, 2), fallback_value, 'PRICED BY MIN MARGIN'\n",
    "\n",
    "# Aplicar la función a cada fila y desempaquetar los resultados en dos nuevas columnas\n",
    "df_stg[['Strategy','new_bench', 'new_price','explanation']] = df_stg.apply(lambda row: pd.Series(formula(row)), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Logic to apply elasticities (B. Suaves)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_info_ryd = df_sheet.get_as_dataframe('elasticity')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bs = pd.merge(df_stg,df_info_ryd,left_on=['source_id'],right_on=['source_id'],how='left')\n",
    "df_bs['uplift'] = 100.00*((df_bs.new_price/df_bs.net_price)-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import percentileofscore\n",
    "\n",
    "# Function to calculate new_uplift for each city\n",
    "def calculate_new_uplift(df):\n",
    "    # Calculate percentiles for each value in the mean_edpv column\n",
    "    percentiles = df['mean_edpv'].apply(lambda x: percentileofscore(df['mean_edpv'], x) / 100.0)\n",
    "    \n",
    "    # Apply the transformation (1 - percentile) and map it to the range [2, 10]\n",
    "    mapped_values = 4 + (1 - percentiles) * (10 - 4)\n",
    "    \n",
    "    # Calculate new_uplift by considering the sign of the original uplift\n",
    "    df['new_uplift'] = np.where(df['uplift'] >= 0, np.minimum(mapped_values, df['uplift']),\n",
    "                                np.maximum(-mapped_values, df['uplift']))\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the calculation for each city separately\n",
    "df_bs = df_bs.groupby('city').apply(calculate_new_uplift)\n",
    "\n",
    "# Handle rows with NaN in mean_edpv separately if needed\n",
    "df_bs_na = df_bs[df_bs['mean_edpv'].isna()].copy()\n",
    "\n",
    "# Adjust the assignment logic to consider the sign of uplift\n",
    "df_bs_na['new_uplift'] = np.where(df_bs_na.cluster == 'KVI', \n",
    "                                  np.where(df_bs_na.uplift >= 0, np.minimum(4, df_bs_na.uplift), np.maximum(-4, df_bs_na.uplift)),\n",
    "                       np.where(df_bs_na.cluster == 'MID', \n",
    "                                np.where(df_bs_na.uplift >= 0, np.minimum(7, df_bs_na.uplift), np.maximum(-7, df_bs_na.uplift)),\n",
    "                       np.where(df_bs_na.uplift >= 0, np.minimum(10, df_bs_na.uplift), np.maximum(-10, df_bs_na.uplift))))\n",
    "\n",
    "# Update the original DataFrame with new_uplift for NaN mean_edpv\n",
    "df_bs.loc[df_bs_na.index, 'new_uplift'] = df_bs_na['new_uplift']\n",
    "\n",
    "# Calculate new_price and round to 2 decimal places\n",
    "df_bs['new_price'] = (df_bs.net_price * (1 + df_bs.new_uplift / 100)).round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If bajadas suaves was applied\n",
    "df_bs['explanation'] = np.where((df_bs['uplift'] != df_bs['new_uplift']) & (df_bs['explanation'] != \"PRICED BY MIN MARGIN\"), \"B. SUAVES\", df_bs['explanation'])\n",
    "\n",
    "# New price with bajadas suaves\n",
    "df_stg['new_price'] = df_bs['new_price']\n",
    "\n",
    "# New price with bajadas suaves\n",
    "df_stg['explanation'] = df_bs['explanation']\n",
    "\n",
    "# Validate strategy of refresh\n",
    "df_stg['Frecuency'] = np.where(df_stg['cluster'] == 'KVI', 'Diaria', 'Semanal')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logica para redondear a .09 los final price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to round to nearest .09\n",
    "def round_to_nearest_09(value):\n",
    "    # Find the integer part and the fractional part\n",
    "    integer_part = np.floor(value)\n",
    "    fractional_part = value - integer_part\n",
    "\n",
    "    # Find the closest multiple of 0.09\n",
    "    rounded_fraction = round(fractional_part / 0.10) * 0.10 - 0.01\n",
    "    \n",
    "    # Combine the integer part with the rounded fractional part\n",
    "    return np.round(integer_part + rounded_fraction, 2)\n",
    "\n",
    "# Apply the function to the DataFrame\n",
    "df_stg['new_price'] = df_stg['new_price'].apply(round_to_nearest_09)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pegar EXPLANATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fix = df_sheet.get_as_dataframe('FIX PRICES')\n",
    "df_fix = df_fix.loc[(df_fix.fecha_inicio <= str(todays_date.date())) & (df_fix.fecha_fin > str(todays_date.date()))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary from df_fix for quick lookup\n",
    "fix_price_dict = df_fix.set_index('source_id')['precio_fijo'].to_dict()\n",
    "\n",
    "# Apply the updates\n",
    "mask = df_stg['source_id'].isin(fix_price_dict.keys())\n",
    "df_stg.loc[mask, 'new_price'] = df_stg.loc[mask, 'source_id'].map(fix_price_dict)\n",
    "df_stg.loc[mask, 'Strategy'] = 'Fix Price'\n",
    "df_stg.loc[mask, 'explanation'] = 'Fix Price CatMan'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aca Para las negociaciones dct proveedor\n",
    "df_stg.loc[mask_prov, 'Strategy'] = 'Negotiation Prov'\n",
    "df_stg.loc[mask_prov, 'explanation'] = 'Fix Price CatMan'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PEGAR INFO\n",
    "df_sheet.set_with_dataframe(df_stg.loc[:, ['source_id','explanation']], worksheet='explanation', clear=True, autocreate=True)\n",
    "\n",
    "# Seleccionar columnas de la 6 a la 9 (índices 5 a 8)\n",
    "# subset_columns = df_print.iloc[:, 5:9]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pegar INFO FINAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PEGAR INFO\n",
    "df_sheet.set_with_dataframe(df_stg.iloc[:, -8:], worksheet='info_to_sheet', clear=True, autocreate=True)\n",
    "\n",
    "# Seleccionar columnas de la 6 a la 9 (índices 5 a 8)\n",
    "# subset_columns = df_print.iloc[:, 5:9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gspread_dataframe as gspread_df\n",
    "from analystcommunity.sheets import print_in_sheets,get_df_sheets,make_connection,get_worksheet_gspread\n",
    "\n",
    "def append_to_last_row(gkey,sheet_name,df,col_init:str = None):\n",
    "    \"\"\"\n",
    "    Function to append a dataframe (without df columns names) to the next row of the last rows of the google sheets from an initial column\n",
    "    \"\"\"\n",
    "    \n",
    "    worksheet=get_worksheet_gspread(gkey,sheet_name)\n",
    "    row_init = len(worksheet.get_all_values())+1\n",
    "    gspread_df.set_with_dataframe(worksheet,df,row = row_init, col =1, include_column_header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stg['fecha'] = str(todays_date)\n",
    "df_resultado = df_stg.loc[:, ['fecha','source_id', 'new_price']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "append_to_last_row('18npT2gawR-Nl7pnAYU290oo5XwCJ9U_PqJenP7nz61A','INFO', df_resultado)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =IFERROR(\n",
    "#   IFS(\n",
    "#     OR(\n",
    "#       AND(AC9<>\"\",OR(AC9>=75%,AC9<=-30%,L9/J9<15%))\n",
    "#     ), \"OUTLIER\",\n",
    "    \n",
    "#     OR(\n",
    "#       OR(R9=\"NO TOCAR\",U9=\"Fix Price\")\n",
    "#     ), \"FIX/NO TOCAR\",\n",
    "\n",
    "#     OR(\n",
    "#       AND(W9<>\"\", C9=\"KVI\", ACS(AC9)<=2%, ACS(AG9)<=5%, AN9>=-15%),\n",
    "#       AND(W9<>\"\", C9<>\"KVI\", ACS(AC9)<=5%),\n",
    "#       AND(C9=\"TAIL\"),\n",
    "#       AND(W9=\"\", C9=\"KVI\", ACS(AG9)<=5%, AN9>=-15%),\n",
    "#       AND(W9=\"\", C9<>\"KVI\")\n",
    "#     ), \"OK\",\n",
    "\n",
    "#     OR(      \n",
    "#       AND(W9<>\"\", ACS(AC9)>5%, ACS(AG9)>5%, AN9<-15%),\n",
    "#       AND(W9=\"\", C9=\"KVI\", ACS(AG9)>5%, AN9<0%),\n",
    "#       AND( C9<>\"TAIL\", AN9<-50%)\n",
    "#     ), \"Alert\",\n",
    "    \n",
    "#     OR(\n",
    "#       AND(W9<>\"\", C9=\"KVI\",OR(ACS(AC9)>2%, ACS(AG9)>5%, AN9>=-100%)),\n",
    "#       AND(W9<>\"\", C9=\"MID\",OR(ACS(AC9)>5%)),\n",
    "#       AND(W9=\"\", C9=\"KVI\")\n",
    "#     ), \"Review\",\n",
    "    \n",
    "#     TRUE, \"Check\"  \n",
    "#   ), \n",
    "#   \"OK\" \n",
    "# )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
